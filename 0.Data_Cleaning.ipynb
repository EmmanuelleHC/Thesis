{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVgG3zZDEQiM"
   },
   "source": [
    "## Join JPMorgan and ICIJ to create User Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import optuna\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iVTNItIYlvsh",
    "outputId": "7f1af788-0d0c-4f16-efa7-9af8f1253fd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1484536 entries, 0 to 1484535\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count    Dtype         \n",
      "---  ------              --------------    -----         \n",
      " 0   Time_step           1484536 non-null  datetime64[ns]\n",
      " 1   Label               1484536 non-null  int64         \n",
      " 2   Transaction_Id      1484536 non-null  object        \n",
      " 3   Sender_Id           1119436 non-null  object        \n",
      " 4   Sender_Account      1119436 non-null  object        \n",
      " 5   Sender_Institution  1119436 non-null  object        \n",
      " 6   Sender_Country      1119436 non-null  object        \n",
      " 7   USD_amount          1484536 non-null  float64       \n",
      " 8   Bene_Id             1169913 non-null  object        \n",
      " 9   Bene_Account        921185 non-null   object        \n",
      " 10  Bene_Institution    921185 non-null   object        \n",
      " 11  Bene_Country        921185 non-null   object        \n",
      " 12  Transaction_Type    1484536 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(10)\n",
      "memory usage: 147.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# File path in a Linux environment\n",
    "file_path = 'Thesis/aml_syn_data.csv'\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Perform the data cleaning operations\n",
    "df['Sender_Country'] = df['Sender_Country'].str.title()\n",
    "df['Bene_Country'] = df['Bene_Country'].str.title()\n",
    "df['Sender_Country'] = df['Sender_Country'].replace({'Usa': 'United States', 'South-Korea': 'South Korea'})\n",
    "df['Bene_Country'] = df['Bene_Country'].replace({'Usa': 'United States', 'South-Korea': 'South Korea'})\n",
    "\n",
    "df['Label'] = df['Label'].replace({'BAD': 1, 'GOOD': 0})\n",
    "\n",
    "# Assuming the dataframe has a column named 'Time_step' for separating the years\n",
    "df['Time_step'] = pd.to_datetime(df['Time_step'])\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "output_file_path = 'Thesis/transactions.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Display DataFrame info\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uDNmf7eZqX-v",
    "outputId": "17021392-c4dd-4bf0-de25-eff1d6639371"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Users per Country and Type:\n",
      "              Country      Type  Unique_User_Count\n",
      "0           Argentina  entities                 81\n",
      "1           Argentina   officer                186\n",
      "2           Australia  entities                 56\n",
      "3           Australia   officer                162\n",
      "4              Canada  entities                 62\n",
      "..                ...       ...                ...\n",
      "61     United-Kingdom   officer              13271\n",
      "62          Venezuela  entities                 70\n",
      "63          Venezuela   officer                235\n",
      "64  Virgin-Islands-Us  entities                 63\n",
      "65  Virgin-Islands-Us   officer                203\n",
      "\n",
      "[66 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_sender = df[['Sender_Country', 'Sender_Id', 'Label']].rename(columns={'Sender_Country': 'Country', 'Sender_Id': 'User_Id'})\n",
    "df_beneficiary = df[['Bene_Country', 'Bene_Id', 'Label']].rename(columns={'Bene_Country': 'Country', 'Bene_Id': 'User_Id'})\n",
    "\n",
    "# Concatenate the sender and beneficiary dataframes\n",
    "df_combined = pd.concat([df_sender, df_beneficiary])\n",
    "\n",
    "# Drop duplicates and NaN values\n",
    "df_combined = df_combined.drop_duplicates().dropna()\n",
    "\n",
    "# Add a Type column based on the ID containing specific substrings\n",
    "def determine_type(id_value):\n",
    "    if pd.isna(id_value):\n",
    "        return 'UNKNOWN'\n",
    "    id_value = str(id_value).upper()\n",
    "    if 'COMPANY' in id_value:\n",
    "        return 'entities'\n",
    "    elif 'OWNER' in id_value or 'CLIENT' in id_value or 'CUSTOMER' in id_value:\n",
    "        return 'officer'\n",
    "    elif 'BILLING' in id_value:\n",
    "        return 'intermediaries'\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "\n",
    "df_combined['Type'] = df_combined['User_Id'].apply(determine_type)\n",
    "\n",
    "# Calculate unique users per country\n",
    "unique_users_per_country = df_combined.groupby(['Country', 'Type'])['User_Id'].nunique().reset_index()\n",
    "unique_users_per_country.rename(columns={'User_Id': 'Unique_User_Count'}, inplace=True)\n",
    "\n",
    "# Display the results\n",
    "print(\"Unique Users per Country and Type:\")\n",
    "print(unique_users_per_country)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SdwJRvm33vIV",
    "outputId": "17debb2e-0ce2-4bc2-e835-1c78aff8e26d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Aggregated Data:\n",
      "                      User_Id        Country  Label      Type\n",
      "0  BILLING-COMPANY-1000026-06  United States      0  entities\n",
      "1  BILLING-COMPANY-1000138-00  United States      1  entities\n",
      "2  BILLING-COMPANY-1000188-04  United States      0  entities\n",
      "0  BILLING-COMPANY-1000282-05  United States      0  entities\n",
      "4  BILLING-COMPANY-1000643-04  United States      0  entities\n",
      "1  BILLING-COMPANY-1000719-05  United States      0  entities\n",
      "6  BILLING-COMPANY-1000913-10  United States      0  entities\n",
      "7  BILLING-COMPANY-1001091-11  United States      0  entities\n",
      "2   BILLING-COMPANY-100157-13  United States      0  entities\n",
      "9  BILLING-COMPANY-1001571-00  United States      1  entities\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the maximum Label status for senders and beneficiaries\n",
    "aggregated_sender_data = df_sender.groupby(['User_Id', 'Country']).agg({\n",
    "    'Label': 'max'\n",
    "}).reset_index()\n",
    "\n",
    "aggregated_beneficiary_data = df_beneficiary.groupby(['User_Id', 'Country']).agg({\n",
    "    'Label': 'max'\n",
    "}).reset_index()\n",
    "\n",
    "# Drop duplicates and NaN values from aggregated data\n",
    "aggregated_sender_data = aggregated_sender_data.drop_duplicates().dropna()\n",
    "aggregated_beneficiary_data = aggregated_beneficiary_data.drop_duplicates().dropna()\n",
    "\n",
    "\n",
    "# Assuming the determine_type function is defined elsewhere and correctly identifies 'Officer'\n",
    "aggregated_sender_data['Type'] = aggregated_sender_data['User_Id'].apply(determine_type)\n",
    "aggregated_beneficiary_data['Type'] = aggregated_beneficiary_data['User_Id'].apply(determine_type)\n",
    "\n",
    "# Combine the aggregated sender and beneficiary dataframes\n",
    "combined_aggregated_data = pd.concat([aggregated_sender_data, aggregated_beneficiary_data])\n",
    "\n",
    "# Sort by User_Id, Country, and Label (in descending order so that 1 comes before 0)\n",
    "combined_aggregated_data = combined_aggregated_data.sort_values(by=['User_Id', 'Country', 'Label'], ascending=[True, True, False])\n",
    "\n",
    "# Drop duplicates, keeping the first occurrence\n",
    "combined_aggregated_data = combined_aggregated_data.drop_duplicates(subset=['User_Id', 'Country'], keep='first')\n",
    "\n",
    "# Display the combined data\n",
    "print(\"Combined Aggregated Data:\")\n",
    "print(combined_aggregated_data.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjbrNyLTA2dz",
    "outputId": "22c24769-77fc-4e04-db6e-3a5260dcbbee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of officers with each label:\n",
      "   Label   Count\n",
      "0      0  177115\n",
      "1      1   44759\n",
      "\n",
      "Sample of the modified combined aggregated data with is_pep flag:\n",
      "                      User_Id        Country  Label      Type  is_pep\n",
      "0  BILLING-COMPANY-1000026-06  United States      0  entities       0\n",
      "1  BILLING-COMPANY-1000138-00  United States      1  entities       0\n",
      "2  BILLING-COMPANY-1000188-04  United States      0  entities       0\n",
      "0  BILLING-COMPANY-1000282-05  United States      0  entities       0\n",
      "4  BILLING-COMPANY-1000643-04  United States      0  entities       0\n",
      "1  BILLING-COMPANY-1000719-05  United States      0  entities       0\n",
      "6  BILLING-COMPANY-1000913-10  United States      0  entities       0\n",
      "7  BILLING-COMPANY-1001091-11  United States      0  entities       0\n",
      "2   BILLING-COMPANY-100157-13  United States      0  entities       0\n",
      "9  BILLING-COMPANY-1001571-00  United States      1  entities       0\n",
      "\n",
      "Overall count of pep status by label:\n",
      "   Label  is_pep   Count\n",
      "0      0       0  159036\n",
      "1      0       1   17415\n",
      "2      1       0   41922\n",
      "3      1       1    4388\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "combined_aggregated_data['is_pep'] = 0\n",
    "\n",
    "# Filter for officers\n",
    "officers_indices = combined_aggregated_data[combined_aggregated_data['Type'] == 'officer'].index\n",
    "\n",
    "# Function to randomly assign is_pep=1 to 10% of the rows\n",
    "def assign_pep_flag(indices):\n",
    "    num_pep = max(1, int(0.1 * len(indices)))  # Ensure at least one row is selected if the subset is small\n",
    "    pep_indices = np.random.choice(indices, num_pep, replace=False)\n",
    "    return pep_indices\n",
    "\n",
    "# Apply the function to the officers indices\n",
    "pep_indices = assign_pep_flag(officers_indices)\n",
    "\n",
    "# Update the is_pep values in the combined_aggregated_data DataFrame\n",
    "combined_aggregated_data.loc[pep_indices, 'is_pep'] = 1\n",
    "\n",
    "# Count the number of officers with each label\n",
    "officers_data = combined_aggregated_data.loc[officers_indices]\n",
    "officers_label_count = officers_data['Label'].value_counts().reset_index()\n",
    "officers_label_count.columns = ['Label', 'Count']\n",
    "\n",
    "# Display the count and a sample of the modified dataframe\n",
    "print(\"Number of officers with each label:\")\n",
    "print(officers_label_count)\n",
    "print(\"\\nSample of the modified combined aggregated data with is_pep flag:\")\n",
    "print(combined_aggregated_data.head(10))\n",
    "overall_pep_count = combined_aggregated_data.groupby(['Label', 'is_pep']).size().reset_index(name='Count')\n",
    "\n",
    "print(\"\\nOverall count of pep status by label:\")\n",
    "print(overall_pep_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzX3iJ2fEpG8",
    "outputId": "28fd3f67-4efc-4c4e-b13d-f035608bcc0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('entities', 'Argentina'): 81,\n",
       " ('officer', 'Argentina'): 186,\n",
       " ('entities', 'Australia'): 56,\n",
       " ('officer', 'Australia'): 162,\n",
       " ('entities', 'Canada'): 62,\n",
       " ('officer', 'Canada'): 186,\n",
       " ('entities', 'Chile'): 60,\n",
       " ('officer', 'Chile'): 182,\n",
       " ('entities', 'China'): 56,\n",
       " ('officer', 'China'): 207,\n",
       " ('entities', 'France'): 64,\n",
       " ('officer', 'France'): 197,\n",
       " ('entities', 'Germany'): 76,\n",
       " ('officer', 'Germany'): 236,\n",
       " ('entities', 'India'): 72,\n",
       " ('officer', 'India'): 234,\n",
       " ('entities', 'Iran'): 2348,\n",
       " ('officer', 'Iran'): 2280,\n",
       " ('entities', 'Isle-Of-Man'): 56,\n",
       " ('officer', 'Isle-Of-Man'): 182,\n",
       " ('entities', 'Israel'): 59,\n",
       " ('officer', 'Israel'): 173,\n",
       " ('entities', 'Italy'): 79,\n",
       " ('officer', 'Italy'): 212,\n",
       " ('entities', 'Luxembourg'): 56,\n",
       " ('officer', 'Luxembourg'): 182,\n",
       " ('entities', 'Marocco'): 74,\n",
       " ('officer', 'Marocco'): 195,\n",
       " ('entities', 'Mexico'): 59,\n",
       " ('officer', 'Mexico'): 194,\n",
       " ('entities', 'Nicaragua'): 59,\n",
       " ('officer', 'Nicaragua'): 183,\n",
       " ('entities', 'Nigeria'): 63,\n",
       " ('officer', 'Nigeria'): 215,\n",
       " ('entities', 'North-Korea'): 2310,\n",
       " ('officer', 'North-Korea'): 2226,\n",
       " ('entities', 'Panama'): 59,\n",
       " ('officer', 'Panama'): 175,\n",
       " ('entities', 'Portugal'): 75,\n",
       " ('officer', 'Portugal'): 227,\n",
       " ('entities', 'Qatar'): 70,\n",
       " ('officer', 'Qatar'): 250,\n",
       " ('entities', 'Russia'): 62,\n",
       " ('officer', 'Russia'): 202,\n",
       " ('entities', 'Singapore'): 63,\n",
       " ('officer', 'Singapore'): 210,\n",
       " ('entities', 'South Korea'): 46,\n",
       " ('officer', 'South Korea'): 177,\n",
       " ('entities', 'South-Africa'): 84,\n",
       " ('officer', 'South-Africa'): 202,\n",
       " ('entities', 'Spain'): 59,\n",
       " ('officer', 'Spain'): 205,\n",
       " ('entities', 'Sweden'): 68,\n",
       " ('officer', 'Sweden'): 220,\n",
       " ('entities', 'Switzerland'): 66,\n",
       " ('officer', 'Switzerland'): 195,\n",
       " ('entities', 'Syria'): 2234,\n",
       " ('officer', 'Syria'): 2276,\n",
       " ('entities', 'United States'): 37493,\n",
       " ('officer', 'United States'): 146679,\n",
       " ('entities', 'United-Kingdom'): 4200,\n",
       " ('officer', 'United-Kingdom'): 13271,\n",
       " ('entities', 'Venezuela'): 70,\n",
       " ('officer', 'Venezuela'): 235,\n",
       " ('entities', 'Virgin-Islands-Us'): 63,\n",
       " ('officer', 'Virgin-Islands-Us'): 203}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_data = combined_aggregated_data.groupby(['Country', 'Type']).size().reset_index(name='Count')\n",
    "\n",
    "# Convert grouped data to dictionary for filtering nodes\n",
    "grouped_dict = {(row['Type'], row['Country']): row['Count'] for _, row in grouped_data.iterrows()}\n",
    "grouped_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7NQRbfupLHg",
    "outputId": "4943aede-a1b2-4a8f-99aa-5bb71b79a5f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3263: DtypeWarning: Columns (4,5,6,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3263: DtypeWarning: Columns (2,3,6,7,10,11,12,13,14,15,16,17,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:262: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities in Argentina: match\n",
      "officer in Argentina: match\n",
      "entities in Australia: match\n",
      "officer in Australia: match\n",
      "entities in Canada: match\n",
      "officer in Canada: match\n",
      "entities in Chile: match\n",
      "officer in Chile: match\n",
      "entities in China: match\n",
      "officer in China: match\n",
      "entities in France: match\n",
      "officer in France: match\n",
      "entities in Germany: match\n",
      "officer in Germany: match\n",
      "entities in India: match\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:272: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:4535: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:262: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "officer in India: match\n",
      "entities in Iran: match\n",
      "officer in Iran: match\n",
      "entities in Isle-Of-Man: match\n",
      "officer in Isle-Of-Man: match\n",
      "entities in Israel: match\n",
      "officer in Israel: match\n",
      "entities in Italy: match\n",
      "officer in Italy: match\n",
      "entities in Luxembourg: match\n",
      "officer in Luxembourg: match\n",
      "entities in Marocco: match\n",
      "officer in Marocco: match\n",
      "entities in Mexico: match\n",
      "officer in Mexico: match\n",
      "entities in Nicaragua: match\n",
      "officer in Nicaragua: match\n",
      "entities in Nigeria: match\n",
      "officer in Nigeria: match\n",
      "entities in North-Korea: match\n",
      "officer in North-Korea: match\n",
      "entities in Panama: match\n",
      "officer in Panama: match\n",
      "entities in Portugal: match\n",
      "officer in Portugal: match\n",
      "entities in Qatar: match\n",
      "officer in Qatar: match\n",
      "entities in Russia: match\n",
      "officer in Russia: match\n",
      "entities in Singapore: match\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:272: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:4535: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:262: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "officer in Singapore: match\n",
      "entities in South Korea: match\n",
      "officer in South Korea: match\n",
      "entities in South-Africa: match\n",
      "officer in South-Africa: match\n",
      "entities in Spain: match\n",
      "officer in Spain: match\n",
      "entities in Sweden: match\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:272: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:4535: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:262: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "officer in Sweden: match\n",
      "entities in Switzerland: match\n",
      "officer in Switzerland: match\n",
      "entities in Syria: match\n",
      "officer in Syria: match\n",
      "entities in United States: match\n",
      "officer in United States: match\n",
      "entities in United-Kingdom: match\n",
      "officer in United-Kingdom: match\n",
      "entities in Venezuela: match\n",
      "officer in Venezuela: match\n",
      "entities in Virgin-Islands-Us: match\n",
      "officer in Virgin-Islands-Us: match\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define file paths\n",
    "base_path = 'Thesis/'\n",
    "relationships_file = base_path + 'relationships.csv'\n",
    "officers_file = base_path + 'nodes-officers.csv'\n",
    "intermediaries_file = base_path + 'nodes-intermediaries.csv'\n",
    "entities_file = base_path + 'nodes-entities.csv'\n",
    "output_nodes_file = base_path + 'usa_nodes.csv'\n",
    "\n",
    "# Function to generate fake entities\n",
    "def generate_fake_entity(entity_type):\n",
    "    fake = Faker()\n",
    "    if entity_type.lower() == 'entities':\n",
    "        return fake.company()\n",
    "    elif entity_type.lower() == 'officer':\n",
    "        return fake.name()\n",
    "    else:\n",
    "        return 'Invalid entity type. Please specify \"company\" or \"person\".'\n",
    "\n",
    "# Function to load and clean data\n",
    "def load_clean_data():\n",
    "    edges = pd.read_csv(relationships_file)\n",
    "    if 'rel_type' in edges.columns:\n",
    "        edges = edges[edges[\"rel_type\"] != \"registrated address\"]\n",
    "\n",
    "    officers = pd.read_csv(officers_file)\n",
    "    officers = officers.dropna(subset=['name', 'country_codes'])\n",
    "    intermediaries = pd.read_csv(intermediaries_file)\n",
    "    intermediaries = intermediaries.dropna(subset=['name', 'countries'])\n",
    "    entities = pd.read_csv(entities_file)\n",
    "    entities = entities.dropna(subset=['name', 'countries'])\n",
    "    entities = entities[~entities['countries'].str.contains(';', na=False)]\n",
    "    entities = entities[entities['country_codes'] != 'XXX']\n",
    "    entities['countries'] = entities['countries'].replace({\n",
    "        'Hong Kong': 'United States',\n",
    "        'Monaco': 'United States',\n",
    "        'United Kingdom': 'United-Kingdom',\n",
    "        'South Africa': 'South-Africa',\n",
    "        'Isle of Man': 'Isle-Of-Man',\n",
    "        'Samoa': 'United States',\n",
    "        'Cayman Islands': 'United States',\n",
    "        'Taiwan': 'North-Korea',\n",
    "        'Peru': 'United States',\n",
    "        'Bermuda': 'United States',\n",
    "        'British Virgin Islands': 'United States',\n",
    "        'U.S. Virgin Islands': 'Virgin-Islands-Us',\n",
    "        'Bahamas': 'Iran',\n",
    "        'Malta': 'Iran',\n",
    "        'Czech Republic': 'United States',\n",
    "        'Seychelles': 'North-Korea',\n",
    "        'Malaysia': 'Marocco',\n",
    "        'Hungary': 'Nicaragua',\n",
    "        'Mauritius': 'Iran',\n",
    "        'Ecuador': 'Syria',\n",
    "        'Barbados': 'Syria',\n",
    "        'Lebanon': 'Syria',\n",
    "        'Singapore': 'China',\n",
    "        'Lithuania': 'North-Korea',\n",
    "        'United Arab Emirates': 'Syria',\n",
    "        'Anguilla': 'Virgin-Islands-Us',\n",
    "        'Cyprus': 'United-Kingdom',\n",
    "        'Cook Islands': 'United-Kingdom',\n",
    "        'Belize': 'United States',\n",
    "        'Bolivia': 'Iran',\n",
    "        'Indonesia': 'India',\n",
    "        'Brazil': 'Syria',\n",
    "        'Denmark': 'Luxembourg',\n",
    "        'Egypt': 'Venezuela',\n",
    "        'Albania':'Italy',\n",
    "        'Ukraine':'Italy',\n",
    "        'Thailand':'Italy',\n",
    "        'Uruguay':'Luxembourg',\n",
    "        'Cayman Island':'Syria',\n",
    "        'Malta':'Qatar',\n",
    "        'Gabon':'Nigeria',\n",
    "        'Poland':'South Korea',\n",
    "        'Costa Rica':'Virgin-Islands-Us',\n",
    "        'Cyprus':'Nigeria'\n",
    "    })\n",
    "\n",
    "    entities['country_codes'] = entities['country_codes'].replace({\n",
    "        'HKG': 'USA',\n",
    "        'MCO': 'USA',\n",
    "        'GBR': 'USA',\n",
    "        'WSM': 'USA',\n",
    "        'CYM': 'USA',\n",
    "        'TWN': 'NK',\n",
    "        'BMU': 'USA',\n",
    "        'VGB': 'USA',\n",
    "        'BHS': 'IRN',\n",
    "        'MLT': 'IRN',\n",
    "        'CZE': 'USA',\n",
    "        'SYC': 'NK',\n",
    "        'MYS': 'MAR',\n",
    "        'HUN': 'NIC',\n",
    "        'MUS': 'IRN',\n",
    "        'ECU': 'SYR',\n",
    "        'BRB': 'SYR',\n",
    "        'LBN': 'SYR',\n",
    "        'SGP': 'CHN',\n",
    "        'LTU': 'NK',\n",
    "        'UAE': 'SYR',\n",
    "        'AIA': 'VGB',\n",
    "        'CYP': 'GBR',\n",
    "        'COK': 'GBR',\n",
    "        'BLZ': 'USA',\n",
    "        'BOL': 'IRN',\n",
    "        'IDN': 'IND',\n",
    "        'AIA':'ITA',\n",
    "        'UKR':'ITA',\n",
    "        'THA':'ITA',\n",
    "        'URY':':LUX',\n",
    "        'CYM':'SYR',\n",
    "        'MLT':'QAT',\n",
    "        'GAB':'NGA',\n",
    "        'POL':'KOR',\n",
    "        'CRI':'VGB',\n",
    "        'CYP':'NGA'\n",
    "    })\n",
    "\n",
    "    officers = officers.drop(columns=['valid_until', 'note', 'sourceID'])\n",
    "    officers = officers[~officers['countries'].str.contains(';', na=False)]\n",
    "    officers = officers[officers['country_codes'] != 'XXX']\n",
    "    officers['countries'] = officers['countries'].replace({\n",
    "        'Hong Kong': 'United States',\n",
    "        'Isle of Man': 'Isle-Of-Man',\n",
    "        'Seychelles': 'North-Korea',\n",
    "        'United Kingdom': 'United-Kingdom',\n",
    "        'South Africa': 'South-Africa',\n",
    "        'Malaysia': 'Marocco',\n",
    "        'Monaco': 'United States',\n",
    "        'Samoa': 'United States',\n",
    "        'Cayman Islands': 'United States',\n",
    "        'Taiwan': 'United States',\n",
    "        'Peru': 'United States',\n",
    "        'Bermuda': 'United States',\n",
    "        'British Virgin Islands': 'United States',\n",
    "        'U.S. Virgin Islands': 'Virgin-Islands-Us',\n",
    "        'Bahamas': 'United States',\n",
    "        'Malta': 'United States',\n",
    "        'Czech Republic': 'United States',\n",
    "        'Hungary': 'Nicaragua',\n",
    "        'Mauritius': 'Iran',\n",
    "        'Ecuador': 'Syria',\n",
    "        'Barbados': 'Syria',\n",
    "        'Lebanon': 'Syria',\n",
    "        'El Salvador': 'Syria',\n",
    "        'Brazil': 'Syria',\n",
    "        'Indonesia': 'Iran'\n",
    "    })\n",
    "\n",
    "    officers['country_codes'] = officers['country_codes'].replace({\n",
    "        'HKG': 'USA',\n",
    "        'SYC': 'NK',\n",
    "        'MYS': 'MAR',\n",
    "        'MCO': 'USA',\n",
    "        'WSM': 'USA',\n",
    "        'CYM': 'USA',\n",
    "        'TWN': 'USA',\n",
    "        'MUS':'IRN',\n",
    "        'IDN':'IRN'\n",
    "    })\n",
    "\n",
    "    intermediaries = intermediaries.drop(columns=['valid_until', 'note', 'sourceID'])\n",
    "    intermediaries = intermediaries[~intermediaries['countries'].str.contains(';', na=False)]\n",
    "    intermediaries = intermediaries[intermediaries['country_codes'] != 'XXX']\n",
    "    intermediaries['countries'] = intermediaries['countries'].replace({\n",
    "        'Malaysia': 'Iran',\n",
    "        'Seychelles': 'North-Korea',\n",
    "        'Hong Kong': 'China',\n",
    "        'Isle of Man': 'Isle-Of-Man',\n",
    "        'United Kingdom': 'United-Kingdom',\n",
    "        'South Africa': 'South-Africa',\n",
    "        'Monaco': 'Iran',\n",
    "        'Luxembourg': 'Iran',\n",
    "        'Austria': 'Luxembourg',\n",
    "        'Samoa': 'North-Korea',\n",
    "        'Cayman Islands': 'Iran',\n",
    "        'Taiwan': 'North-Korea',\n",
    "        'Peru': 'Syria',\n",
    "        'Bermuda': 'Syria',\n",
    "        'Hungary': 'Iran',\n",
    "        'Mauritius': 'Iran',\n",
    "        'Bahamas': 'Iran',\n",
    "        'Malta': 'Iran',\n",
    "        'Czech Republic': 'North-Korea',\n",
    "        'Ecuador': 'Syria',\n",
    "        'Barbados': 'Syria',\n",
    "        'Lebanon': 'Syria',\n",
    "        'Andorra': 'India',\n",
    "        'Aruba': 'Virgin-Islands-Us',\n",
    "        'Cuba': 'Qatar',\n",
    "        'China': 'North-Korea',\n",
    "        'United States': 'Syria',\n",
    "        'Venezuela': 'Syria'\n",
    "    })\n",
    "\n",
    "    intermediaries['country_codes'] = intermediaries['country_codes'].replace({\n",
    "        'MYS': 'IRN',\n",
    "        'SYC': 'NK',\n",
    "        'HKG': 'CHN',\n",
    "        'MCO': 'IRN',\n",
    "        'LUX': 'IRN',\n",
    "        'AUT': 'LUX',\n",
    "        'WSM': 'NK',\n",
    "        'CYM': 'IRN',\n",
    "        'TWN': 'NK',\n",
    "        'PER': 'SYR',\n",
    "        'BMU': 'SYR',\n",
    "        'HUN': 'IRN',\n",
    "        'MUS': 'IRN',\n",
    "        'BHS': 'IRN',\n",
    "        'MLT': 'IRN',\n",
    "        'CZE': 'NK',\n",
    "        'ECU': 'SYR',\n",
    "        'BRB': 'SYR',\n",
    "        'LBN': 'SYR',\n",
    "        'AND': 'IND',\n",
    "        'ABW': 'VGB',\n",
    "        'CUB': 'QAT',\n",
    "        'CHN': 'NK',\n",
    "        'USA': 'SYR',\n",
    "        'VEN': 'SYR',\n",
    "        'BRA': 'SYR',\n",
    "        'DNK': 'LUX',\n",
    "        'EGY': 'VEN',\n",
    "        'BOL': 'VGB'\n",
    "    })\n",
    "\n",
    "    officers['type'] = \"officer\"\n",
    "    intermediaries['type'] = \"intermediary\"\n",
    "    entities['type'] = \"entities\"\n",
    "\n",
    "    columns = ['node_id', 'name','type', 'countries', 'country_codes']\n",
    "\n",
    "    officers = officers[columns]\n",
    "    intermediaries = intermediaries[columns]\n",
    "    entities = entities[columns]\n",
    "\n",
    "    all_nodes = pd.concat([officers, intermediaries, entities]).reset_index(drop=True)\n",
    "\n",
    "    all_nodes[\"name\"] = all_nodes[\"name\"].str.upper().str.replace(' ', '_')\n",
    "    all_nodes[\"name\"].replace(\n",
    "        to_replace=[r\"MRS?\\.\\s+\", r\"\\.\", r\"\\s+\", \"LIMITED\", \"THE BEARER\", \"BEARER\", \"BEARER 1\", \"EL PORTADOR\", \"AL PORTADOR\"],\n",
    "        value=[\"\", \"\", \"\", \"LTD\", np.nan, np.nan, np.nan, np.nan, np.nan],\n",
    "        inplace=True, regex=True)\n",
    "    all_nodes = all_nodes[~all_nodes.index.duplicated(keep='first')]\n",
    "    all_nodes = all_nodes[all_nodes['name'].str.strip() != '']\n",
    "    all_nodes['countries'] = all_nodes['countries'].str.title()\n",
    "\n",
    "    return all_nodes, edges\n",
    "\n",
    "def filter_save_usa_and_foreign_nodes(grouped_dict, all_nodes):\n",
    "    OUTPUT_NODES_FILE = '/content/drive/My Drive/Thesis/DataCleaning/usa_nodes.csv'\n",
    "    all_nodes['type'] = all_nodes['type'].replace('entity', 'entities')\n",
    "    filtered_nodes_list = []\n",
    "\n",
    "    for (entity_type, country), count in grouped_dict.items():\n",
    "        entity_type_lower = entity_type.lower()\n",
    "        filtered_nodes = all_nodes[(all_nodes['type'] == entity_type_lower) & (all_nodes['countries'] == country)]\n",
    "\n",
    "        # Replace NaN values in 'name' with an empty string for length check\n",
    "        filtered_nodes['name'] = filtered_nodes['name'].fillna('')\n",
    "\n",
    "        # Generate fake entities for names with length <= 3\n",
    "        filtered_nodes['name'] = filtered_nodes['name'].apply(\n",
    "            lambda name: generate_fake_entity(entity_type_lower) if len(name) <= 3 else name\n",
    "        )\n",
    "\n",
    "        # If filtered_nodes is empty and the entity_type is 'entities', use intermediaries\n",
    "        if len(filtered_nodes) < count and entity_type_lower == 'entities':\n",
    "            intermediaries_as_entities = all_nodes[(all_nodes['type'] == 'intermediary') & (all_nodes['countries'] == country)]\n",
    "            intermediaries_as_entities['type'] = 'entities'\n",
    "            # Fill NaN values in the 'name' column with an empty string\n",
    "            intermediaries_as_entities['name'].fillna('', inplace=True)\n",
    "\n",
    "            # Use .loc to apply the function to rows where the length of 'name' is less than or equal to 3\n",
    "            intermediaries_as_entities.loc[intermediaries_as_entities['name'].str.len() <= 3, 'name'] = intermediaries_as_entities.loc[intermediaries_as_entities['name'].str.len() <= 3, 'name'].apply(lambda name: generate_fake_entity('entities'))\n",
    "\n",
    "            filtered_nodes = pd.concat([filtered_nodes, intermediaries_as_entities])\n",
    "\n",
    "        # If more nodes are found than needed, sample the specified count\n",
    "        if len(filtered_nodes) >= count:\n",
    "            filtered_nodes = filtered_nodes.sample(n=count, random_state=42)\n",
    "\n",
    "        filtered_nodes_list.append(filtered_nodes)\n",
    "\n",
    "        # Check if the actual count matches the expected count\n",
    "        if len(filtered_nodes) >= count:\n",
    "            print(f\"{entity_type} in {country}: match\")\n",
    "        else:\n",
    "            print(f\"{entity_type} in {country}: not match (Needed = {count}, Available = {len(filtered_nodes)})\")\n",
    "\n",
    "    # Concatenate all filtered nodes into a single DataFrame\n",
    "    all_filtered_nodes = pd.concat(filtered_nodes_list)\n",
    "    return all_filtered_nodes\n",
    "\n",
    "# Assuming 'grouped_data' is already defined as a DataFrame\n",
    "grouped_dict = {(row['Type'], row['Country']): row['Count'] for _, row in grouped_data.iterrows()}\n",
    "\n",
    "# Load data\n",
    "all_nodes, edges = load_clean_data()\n",
    "\n",
    "# Filter and save nodes\n",
    "filtered_nodes = filter_save_usa_and_foreign_nodes(grouped_dict, all_nodes)\n",
    "filtered_nodes.to_csv(output_nodes_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b4N2gDuvdXD",
    "outputId": "3e03200a-d1a9-4c71-f9f3-91d60efb6c42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique country-type combinations in nodes_pd: [{'Country': 'United States', 'Type': 'entities'}, {'Country': 'United-Kingdom', 'Type': 'entities'}, {'Country': 'Italy', 'Type': 'entities'}, {'Country': 'Nicaragua', 'Type': 'entities'}, {'Country': 'Panama', 'Type': 'entities'}, {'Country': 'Luxembourg', 'Type': 'entities'}, {'Country': 'France', 'Type': 'entities'}, {'Country': 'South-Africa', 'Type': 'entities'}, {'Country': 'Singapore', 'Type': 'entities'}, {'Country': 'Portugal', 'Type': 'entities'}, {'Country': 'Qatar', 'Type': 'entities'}, {'Country': 'Canada', 'Type': 'entities'}, {'Country': 'Germany', 'Type': 'entities'}, {'Country': 'India', 'Type': 'entities'}, {'Country': 'China', 'Type': 'entities'}, {'Country': 'Spain', 'Type': 'entities'}, {'Country': 'Australia', 'Type': 'entities'}, {'Country': 'Israel', 'Type': 'entities'}, {'Country': 'Virgin-Islands-Us', 'Type': 'entities'}, {'Country': 'Isle-Of-Man', 'Type': 'entities'}, {'Country': 'Mexico', 'Type': 'entities'}, {'Country': 'Venezuela', 'Type': 'entities'}, {'Country': 'Sweden', 'Type': 'entities'}, {'Country': 'Marocco', 'Type': 'entities'}, {'Country': 'Nigeria', 'Type': 'entities'}, {'Country': 'South Korea', 'Type': 'entities'}, {'Country': 'Chile', 'Type': 'entities'}, {'Country': 'Russia', 'Type': 'entities'}, {'Country': 'Argentina', 'Type': 'entities'}, {'Country': 'Switzerland', 'Type': 'entities'}, {'Country': 'United States', 'Type': 'officer'}, {'Country': 'Syria', 'Type': 'officer'}, {'Country': 'United-Kingdom', 'Type': 'officer'}, {'Country': 'North-Korea', 'Type': 'officer'}, {'Country': 'Iran', 'Type': 'officer'}, {'Country': 'Panama', 'Type': 'officer'}, {'Country': 'Italy', 'Type': 'officer'}, {'Country': 'India', 'Type': 'officer'}, {'Country': 'Nicaragua', 'Type': 'officer'}, {'Country': 'Portugal', 'Type': 'officer'}, {'Country': 'Isle-Of-Man', 'Type': 'officer'}, {'Country': 'Sweden', 'Type': 'officer'}, {'Country': 'Marocco', 'Type': 'officer'}, {'Country': 'Australia', 'Type': 'officer'}, {'Country': 'France', 'Type': 'officer'}, {'Country': 'Canada', 'Type': 'officer'}, {'Country': 'Singapore', 'Type': 'officer'}, {'Country': 'Virgin-Islands-Us', 'Type': 'officer'}, {'Country': 'Luxembourg', 'Type': 'officer'}, {'Country': 'Chile', 'Type': 'officer'}, {'Country': 'Mexico', 'Type': 'officer'}, {'Country': 'Qatar', 'Type': 'officer'}, {'Country': 'Argentina', 'Type': 'officer'}, {'Country': 'Switzerland', 'Type': 'officer'}, {'Country': 'Germany', 'Type': 'officer'}, {'Country': 'China', 'Type': 'officer'}, {'Country': 'Israel', 'Type': 'officer'}, {'Country': 'South-Africa', 'Type': 'officer'}, {'Country': 'Nigeria', 'Type': 'officer'}, {'Country': 'Spain', 'Type': 'officer'}, {'Country': 'South Korea', 'Type': 'officer'}, {'Country': 'Venezuela', 'Type': 'officer'}, {'Country': 'Russia', 'Type': 'officer'}, {'Country': 'North-Korea', 'Type': 'entities'}, {'Country': 'Syria', 'Type': 'entities'}, {'Country': 'Iran', 'Type': 'entities'}]\n",
      "Processing country: United States, type: entities\n",
      "Number of individuals in United States of type entities: 37493\n",
      "Number of available nodes in United States of type entities: 37493\n",
      "Processing country: United-Kingdom, type: entities\n",
      "Number of individuals in United-Kingdom of type entities: 4200\n",
      "Number of available nodes in United-Kingdom of type entities: 4200\n",
      "Processing country: Italy, type: entities\n",
      "Number of individuals in Italy of type entities: 79\n",
      "Number of available nodes in Italy of type entities: 79\n",
      "Processing country: Nicaragua, type: entities\n",
      "Number of individuals in Nicaragua of type entities: 59\n",
      "Number of available nodes in Nicaragua of type entities: 59\n",
      "Processing country: Panama, type: entities\n",
      "Number of individuals in Panama of type entities: 59\n",
      "Number of available nodes in Panama of type entities: 59\n",
      "Processing country: Luxembourg, type: entities\n",
      "Number of individuals in Luxembourg of type entities: 56\n",
      "Number of available nodes in Luxembourg of type entities: 56\n",
      "Processing country: France, type: entities\n",
      "Number of individuals in France of type entities: 64\n",
      "Number of available nodes in France of type entities: 64\n",
      "Processing country: South-Africa, type: entities\n",
      "Number of individuals in South-Africa of type entities: 84\n",
      "Number of available nodes in South-Africa of type entities: 84\n",
      "Processing country: Singapore, type: entities\n",
      "Number of individuals in Singapore of type entities: 63\n",
      "Number of available nodes in Singapore of type entities: 63\n",
      "Processing country: Portugal, type: entities\n",
      "Number of individuals in Portugal of type entities: 75\n",
      "Number of available nodes in Portugal of type entities: 75\n",
      "Processing country: Qatar, type: entities\n",
      "Number of individuals in Qatar of type entities: 70\n",
      "Number of available nodes in Qatar of type entities: 70\n",
      "Processing country: Canada, type: entities\n",
      "Number of individuals in Canada of type entities: 62\n",
      "Number of available nodes in Canada of type entities: 62\n",
      "Processing country: Germany, type: entities\n",
      "Number of individuals in Germany of type entities: 76\n",
      "Number of available nodes in Germany of type entities: 76\n",
      "Processing country: India, type: entities\n",
      "Number of individuals in India of type entities: 72\n",
      "Number of available nodes in India of type entities: 72\n",
      "Processing country: China, type: entities\n",
      "Number of individuals in China of type entities: 56\n",
      "Number of available nodes in China of type entities: 56\n",
      "Processing country: Spain, type: entities\n",
      "Number of individuals in Spain of type entities: 59\n",
      "Number of available nodes in Spain of type entities: 59\n",
      "Processing country: Australia, type: entities\n",
      "Number of individuals in Australia of type entities: 56\n",
      "Number of available nodes in Australia of type entities: 56\n",
      "Processing country: Israel, type: entities\n",
      "Number of individuals in Israel of type entities: 59\n",
      "Number of available nodes in Israel of type entities: 59\n",
      "Processing country: Virgin-Islands-Us, type: entities\n",
      "Number of individuals in Virgin-Islands-Us of type entities: 63\n",
      "Number of available nodes in Virgin-Islands-Us of type entities: 63\n",
      "Processing country: Isle-Of-Man, type: entities\n",
      "Number of individuals in Isle-Of-Man of type entities: 56\n",
      "Number of available nodes in Isle-Of-Man of type entities: 56\n",
      "Processing country: Mexico, type: entities\n",
      "Number of individuals in Mexico of type entities: 59\n",
      "Number of available nodes in Mexico of type entities: 59\n",
      "Processing country: Venezuela, type: entities\n",
      "Number of individuals in Venezuela of type entities: 70\n",
      "Number of available nodes in Venezuela of type entities: 70\n",
      "Processing country: Sweden, type: entities\n",
      "Number of individuals in Sweden of type entities: 68\n",
      "Number of available nodes in Sweden of type entities: 68\n",
      "Processing country: Marocco, type: entities\n",
      "Number of individuals in Marocco of type entities: 74\n",
      "Number of available nodes in Marocco of type entities: 74\n",
      "Processing country: Nigeria, type: entities\n",
      "Number of individuals in Nigeria of type entities: 63\n",
      "Number of available nodes in Nigeria of type entities: 63\n",
      "Processing country: South Korea, type: entities\n",
      "Number of individuals in South Korea of type entities: 46\n",
      "Number of available nodes in South Korea of type entities: 46\n",
      "Processing country: Chile, type: entities\n",
      "Number of individuals in Chile of type entities: 60\n",
      "Number of available nodes in Chile of type entities: 60\n",
      "Processing country: Russia, type: entities\n",
      "Number of individuals in Russia of type entities: 62\n",
      "Number of available nodes in Russia of type entities: 62\n",
      "Processing country: Argentina, type: entities\n",
      "Number of individuals in Argentina of type entities: 81\n",
      "Number of available nodes in Argentina of type entities: 81\n",
      "Processing country: Switzerland, type: entities\n",
      "Number of individuals in Switzerland of type entities: 66\n",
      "Number of available nodes in Switzerland of type entities: 66\n",
      "Processing country: United States, type: officer\n",
      "Number of individuals in United States of type officer: 146679\n",
      "Number of available nodes in United States of type officer: 146679\n",
      "Processing country: Syria, type: officer\n",
      "Number of individuals in Syria of type officer: 2276\n",
      "Number of available nodes in Syria of type officer: 2276\n",
      "Processing country: United-Kingdom, type: officer\n",
      "Number of individuals in United-Kingdom of type officer: 13271\n",
      "Number of available nodes in United-Kingdom of type officer: 13271\n",
      "Processing country: North-Korea, type: officer\n",
      "Number of individuals in North-Korea of type officer: 2226\n",
      "Number of available nodes in North-Korea of type officer: 2226\n",
      "Processing country: Iran, type: officer\n",
      "Number of individuals in Iran of type officer: 2280\n",
      "Number of available nodes in Iran of type officer: 2280\n",
      "Processing country: Panama, type: officer\n",
      "Number of individuals in Panama of type officer: 175\n",
      "Number of available nodes in Panama of type officer: 175\n",
      "Processing country: Italy, type: officer\n",
      "Number of individuals in Italy of type officer: 212\n",
      "Number of available nodes in Italy of type officer: 212\n",
      "Processing country: India, type: officer\n",
      "Number of individuals in India of type officer: 234\n",
      "Number of available nodes in India of type officer: 233\n",
      "No available nodes left for country: India, type: officer\n",
      "Processing country: Nicaragua, type: officer\n",
      "Number of individuals in Nicaragua of type officer: 183\n",
      "Number of available nodes in Nicaragua of type officer: 183\n",
      "Processing country: Portugal, type: officer\n",
      "Number of individuals in Portugal of type officer: 227\n",
      "Number of available nodes in Portugal of type officer: 227\n",
      "Processing country: Isle-Of-Man, type: officer\n",
      "Number of individuals in Isle-Of-Man of type officer: 182\n",
      "Number of available nodes in Isle-Of-Man of type officer: 182\n",
      "Processing country: Sweden, type: officer\n",
      "Number of individuals in Sweden of type officer: 220\n",
      "Number of available nodes in Sweden of type officer: 220\n",
      "Processing country: Marocco, type: officer\n",
      "Number of individuals in Marocco of type officer: 195\n",
      "Number of available nodes in Marocco of type officer: 195\n",
      "Processing country: Australia, type: officer\n",
      "Number of individuals in Australia of type officer: 162\n",
      "Number of available nodes in Australia of type officer: 162\n",
      "Processing country: France, type: officer\n",
      "Number of individuals in France of type officer: 197\n",
      "Number of available nodes in France of type officer: 197\n",
      "Processing country: Canada, type: officer\n",
      "Number of individuals in Canada of type officer: 186\n",
      "Number of available nodes in Canada of type officer: 186\n",
      "Processing country: Singapore, type: officer\n",
      "Number of individuals in Singapore of type officer: 210\n",
      "Number of available nodes in Singapore of type officer: 210\n",
      "Processing country: Virgin-Islands-Us, type: officer\n",
      "Number of individuals in Virgin-Islands-Us of type officer: 203\n",
      "Number of available nodes in Virgin-Islands-Us of type officer: 203\n",
      "Processing country: Luxembourg, type: officer\n",
      "Number of individuals in Luxembourg of type officer: 182\n",
      "Number of available nodes in Luxembourg of type officer: 182\n",
      "Processing country: Chile, type: officer\n",
      "Number of individuals in Chile of type officer: 182\n",
      "Number of available nodes in Chile of type officer: 182\n",
      "Processing country: Mexico, type: officer\n",
      "Number of individuals in Mexico of type officer: 194\n",
      "Number of available nodes in Mexico of type officer: 194\n",
      "Processing country: Qatar, type: officer\n",
      "Number of individuals in Qatar of type officer: 250\n",
      "Number of available nodes in Qatar of type officer: 250\n",
      "Processing country: Argentina, type: officer\n",
      "Number of individuals in Argentina of type officer: 186\n",
      "Number of available nodes in Argentina of type officer: 186\n",
      "Processing country: Switzerland, type: officer\n",
      "Number of individuals in Switzerland of type officer: 195\n",
      "Number of available nodes in Switzerland of type officer: 195\n",
      "Processing country: Germany, type: officer\n",
      "Number of individuals in Germany of type officer: 236\n",
      "Number of available nodes in Germany of type officer: 236\n",
      "Processing country: China, type: officer\n",
      "Number of individuals in China of type officer: 207\n",
      "Number of available nodes in China of type officer: 207\n",
      "Processing country: Israel, type: officer\n",
      "Number of individuals in Israel of type officer: 173\n",
      "Number of available nodes in Israel of type officer: 173\n",
      "Processing country: South-Africa, type: officer\n",
      "Number of individuals in South-Africa of type officer: 202\n",
      "Number of available nodes in South-Africa of type officer: 202\n",
      "Processing country: Nigeria, type: officer\n",
      "Number of individuals in Nigeria of type officer: 215\n",
      "Number of available nodes in Nigeria of type officer: 215\n",
      "Processing country: Spain, type: officer\n",
      "Number of individuals in Spain of type officer: 205\n",
      "Number of available nodes in Spain of type officer: 205\n",
      "Processing country: South Korea, type: officer\n",
      "Number of individuals in South Korea of type officer: 177\n",
      "Number of available nodes in South Korea of type officer: 177\n",
      "Processing country: Venezuela, type: officer\n",
      "Number of individuals in Venezuela of type officer: 235\n",
      "Number of available nodes in Venezuela of type officer: 235\n",
      "Processing country: Russia, type: officer\n",
      "Number of individuals in Russia of type officer: 202\n",
      "Number of available nodes in Russia of type officer: 202\n",
      "Processing country: North-Korea, type: entities\n",
      "Number of individuals in North-Korea of type entities: 2310\n",
      "Number of available nodes in North-Korea of type entities: 2310\n",
      "Processing country: Syria, type: entities\n",
      "Number of individuals in Syria of type entities: 2234\n",
      "Number of available nodes in Syria of type entities: 2234\n",
      "Processing country: Iran, type: entities\n",
      "Number of individuals in Iran of type entities: 2348\n",
      "Number of available nodes in Iran of type entities: 2348\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_mappings(filtered_df, nodes_pd, used_nodes):\n",
    "    mapping_list = []\n",
    "\n",
    "    # Rename columns for consistency\n",
    "    filtered_df = filtered_df.rename(columns={'countries': 'Country', 'type': 'Type'})\n",
    "    nodes_pd = nodes_pd.rename(columns={ 'Type': 'Type', 'Country': 'Country'})\n",
    "\n",
    "    # Get unique country-type combinations\n",
    "    country_type_combinations = nodes_pd[['Country', 'Type']].drop_duplicates().to_dict('records')\n",
    "    print(f\"Unique country-type combinations in nodes_pd: {country_type_combinations}\")\n",
    "\n",
    "    # Convert nodes to a dictionary for faster lookups\n",
    "    nodes_dict = filtered_df.to_dict('records')\n",
    "    nodes_by_country_type = {}\n",
    "    for node in nodes_dict:\n",
    "        country_type_key = (node['Country'], node['Type'])\n",
    "        if country_type_key not in nodes_by_country_type:\n",
    "            nodes_by_country_type[country_type_key] = []\n",
    "        nodes_by_country_type[country_type_key].append(node)\n",
    "\n",
    "    for combo in country_type_combinations:\n",
    "        country = combo['Country']\n",
    "        type_ = combo['Type']\n",
    "        country_type_key = (country, type_)\n",
    "\n",
    "        # Filter individuals by country and type\n",
    "        country_type_individuals_df = nodes_pd[(nodes_pd['Country'] == country) & (nodes_pd['Type'] == type_)]\n",
    "\n",
    "        # Get nodes for the country and type, excluding already used nodes\n",
    "        available_nodes = [node for node in nodes_by_country_type.get(country_type_key, []) if node['node_id'] not in used_nodes]\n",
    "\n",
    "        print(f\"Processing country: {country}, type: {type_}\")\n",
    "        print(f\"Number of individuals in {country} of type {type_}: {len(country_type_individuals_df)}\")\n",
    "        print(f\"Number of available nodes in {country} of type {type_}: {len(available_nodes)}\")\n",
    "\n",
    "        if available_nodes:\n",
    "            # Sample nodes for each customer\n",
    "            for _, customer_row in country_type_individuals_df.iterrows():\n",
    "                if available_nodes:\n",
    "                    sampled_node = available_nodes.pop(0)\n",
    "                    node_id = sampled_node['node_id']\n",
    "\n",
    "                    # Mark the node as used\n",
    "                    used_nodes.add(node_id)\n",
    "\n",
    "                    # Map Customer_Id to the sampled node\n",
    "                    customer_id = customer_row['User_Id']\n",
    "                    mapping_list.append({\n",
    "                        'User_Id': customer_id,\n",
    "                        'node_id': sampled_node['node_id'],\n",
    "                        'name': sampled_node['name'],\n",
    "                        'type': sampled_node['Type'],\n",
    "                        'country_codes': sampled_node['country_codes'],\n",
    "                        'countries': sampled_node['Country'],\n",
    "                        'is_pep': customer_row['is_pep']\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"No available nodes left for country: {country}, type: {type_}\")\n",
    "        else:\n",
    "            print(f\"No nodes available for country: {country}, type: {type_}\")\n",
    "\n",
    "    return mapping_list\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_nodes)\n",
    "nodes_pd = pd.DataFrame(combined_aggregated_data)\n",
    "used_nodes = set()\n",
    "\n",
    "# Create mappings\n",
    "mappings = create_mappings(filtered_df, nodes_pd, used_nodes)\n",
    "mappings_df = pd.DataFrame(mappings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tf1Ls2zk-WcH",
    "outputId": "d58f604b-ee28-4b7f-acc6-fb6b1022bd94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    account_number   node_id  \\\n",
      "0       BILLING-COMPANY-1000026-06  10206359   \n",
      "1       BILLING-COMPANY-1000138-00  10023735   \n",
      "2       BILLING-COMPANY-1000188-04    163868   \n",
      "3       BILLING-COMPANY-1000282-05  10057656   \n",
      "4       BILLING-COMPANY-1000643-04    156515   \n",
      "...                            ...       ...   \n",
      "222755  STANDARD-COMPANY-993341-00  30019278   \n",
      "222756  STANDARD-COMPANY-996272-00  30020097   \n",
      "222757   STANDARD-COMPANY-99731-00  30009231   \n",
      "222758   STANDARD-COMPANY-99756-00  30014526   \n",
      "222759  STANDARD-COMPANY-999187-00  30020943   \n",
      "\n",
      "                                              name      type country_codes  \\\n",
      "0       KINGSTONE_INTERNATIONAL_INVESTMENT_CO,_LTD  entities           USA   \n",
      "1                             BM_ENTERTAINMENT_LTD  entities           USA   \n",
      "2                ALYSSON_INTERNATIONAL_CORPORATION  entities           USA   \n",
      "3                                       TAMWIN_LTD  entities           USA   \n",
      "4                              TREASURE_ACCESS_LTD  entities           USA   \n",
      "...                                            ...       ...           ...   \n",
      "222755                     YSLAND_COMPANY_LTD,_THE  entities           IRN   \n",
      "222756                         DHARMA_HOLDINGS_LTD  entities           IRN   \n",
      "222757                           BAHAMA_-_INCO_LTD  entities           IRN   \n",
      "222758                               PEARLYTON_LTD  entities           IRN   \n",
      "222759                     DARNLEY_INVESTMENTS_LTD  entities           IRN   \n",
      "\n",
      "            countries  is_pep  customer_id  \n",
      "0       United States       0       524296  \n",
      "1       United States       0       524299  \n",
      "2       United States       0       524304  \n",
      "3       United States       0       524311  \n",
      "4       United States       0       524314  \n",
      "...               ...     ...          ...  \n",
      "222755           Iran       0       524275  \n",
      "222756           Iran       0       524279  \n",
      "222757           Iran       0       524280  \n",
      "222758           Iran       0       524286  \n",
      "222759           Iran       0       524287  \n",
      "\n",
      "[222760 rows x 8 columns]\n",
      "Mappings DataFrame saved to 'mappings.csv'.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "mappings_df = mappings_df.rename(columns={'User_Id': 'account_number'})\n",
    "\n",
    "# Function to generate unique 6-digit User_Id\n",
    "def generate_unique_ids(n):\n",
    "    ids = set()\n",
    "    while len(ids) < n:\n",
    "        new_id = random.randint(100000, 999999)\n",
    "        ids.add(new_id)\n",
    "    return list(ids)\n",
    "\n",
    "# Generate unique 6-digit User_Id values\n",
    "num_records = len(mappings_df)\n",
    "new_user_ids = generate_unique_ids(num_records)\n",
    "# mappings_df = mappings_df.drop(columns=['user_Id'])\n",
    "\n",
    "# Assign new User_Id values to the DataFrame\n",
    "mappings_df['customer_id'] = new_user_ids\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(mappings_df)\n",
    "mappings_df.to_csv('Thesis/mappings.csv', index=False)\n",
    "print(\"Mappings DataFrame saved to 'mappings.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zEhsFA8lEodf",
    "outputId": "012df9fe-ee6a-48d8-c756-5e7ab72ef7a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Users: 222760\n",
      "PEP Counts:\n",
      " 0    200957\n",
      "1     21803\n",
      "Name: is_pep, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "total_users = len(mappings_df)\n",
    "print(\"Total Number of Users:\", total_users)\n",
    "\n",
    "# Count the number of PEPs (is_pep = 1) and non-PEPs (is_pep = 0)\n",
    "pep_counts = mappings_df['is_pep'].value_counts()\n",
    "print(\"PEP Counts:\\n\", pep_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RIsV3ncHIse"
   },
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j9CTy0n2HLaA",
    "outputId": "830bf187-9cac-4b34-ba88-ce74ffe26cad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    account_number   node_id  \\\n",
      "0       BILLING-COMPANY-1000026-06  10206359   \n",
      "1       BILLING-COMPANY-1000138-00  10023735   \n",
      "2       BILLING-COMPANY-1000188-04    163868   \n",
      "3       BILLING-COMPANY-1000282-05  10057656   \n",
      "4       BILLING-COMPANY-1000643-04    156515   \n",
      "...                            ...       ...   \n",
      "222755  STANDARD-COMPANY-993341-00  30019278   \n",
      "222756  STANDARD-COMPANY-996272-00  30020097   \n",
      "222757   STANDARD-COMPANY-99731-00  30009231   \n",
      "222758   STANDARD-COMPANY-99756-00  30014526   \n",
      "222759  STANDARD-COMPANY-999187-00  30020943   \n",
      "\n",
      "                                              name      type country_codes  \\\n",
      "0       KINGSTONE_INTERNATIONAL_INVESTMENT_CO,_LTD  entities           USA   \n",
      "1                             BM_ENTERTAINMENT_LTD  entities           USA   \n",
      "2                ALYSSON_INTERNATIONAL_CORPORATION  entities           USA   \n",
      "3                                       TAMWIN_LTD  entities           USA   \n",
      "4                              TREASURE_ACCESS_LTD  entities           USA   \n",
      "...                                            ...       ...           ...   \n",
      "222755                     YSLAND_COMPANY_LTD,_THE  entities           IRN   \n",
      "222756                         DHARMA_HOLDINGS_LTD  entities           IRN   \n",
      "222757                           BAHAMA_-_INCO_LTD  entities           IRN   \n",
      "222758                               PEARLYTON_LTD  entities           IRN   \n",
      "222759                     DARNLEY_INVESTMENTS_LTD  entities           IRN   \n",
      "\n",
      "            countries  is_pep  customer_id  \\\n",
      "0       United States       0       524296   \n",
      "1       United States       0       524299   \n",
      "2       United States       0       524304   \n",
      "3       United States       0       524311   \n",
      "4       United States       0       524314   \n",
      "...               ...     ...          ...   \n",
      "222755           Iran       0       524275   \n",
      "222756           Iran       0       524279   \n",
      "222757           Iran       0       524280   \n",
      "222758           Iran       0       524286   \n",
      "222759           Iran       0       524287   \n",
      "\n",
      "                                  name_masked       account_number_masked  \n",
      "0         Hebert, Johnson and Williams LTD 56   BILLING-COMPANY-7853-7874  \n",
      "1              Meza, Frazier and Brown LTD 50   BILLING-COMPANY-4345-3113  \n",
      "2                     Mueller and Sons LTD 25   BILLING-COMPANY-9439-5445  \n",
      "3       Patterson, Coleman and Hubbard LTD 36   BILLING-COMPANY-1249-6424  \n",
      "4                            Davis Ltd LTD 68   BILLING-COMPANY-5875-5265  \n",
      "...                                       ...                         ...  \n",
      "222755                    Kelly-Mathis LTD 64  STANDARD-COMPANY-6314-8225  \n",
      "222756           Beck, Perry and Bates LTD 56  STANDARD-COMPANY-9040-9153  \n",
      "222757                      Landry Inc LTD 13  STANDARD-COMPANY-9606-7529  \n",
      "222758       Hines, Marquez and Graves LTD 20  STANDARD-COMPANY-9203-9269  \n",
      "222759                   Herman-Snyder LTD 61  STANDARD-COMPANY-5258-4946  \n",
      "\n",
      "[222760 rows x 10 columns]\n",
      "Masked mappings DataFrame saved to 'masked_mappings.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mappings_df = pd.read_csv('Thesis/mappings.csv')\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Sets to keep track of generated names to ensure uniqueness\n",
    "generated_company_names = set()\n",
    "generated_person_names = set()\n",
    "\n",
    "# Function to generate a unique company name with a two-digit suffix\n",
    "def generate_unique_company_name():\n",
    "    name = fake.company() + \" LTD \" + str(random.randint(10, 99))\n",
    "    while name in generated_company_names:\n",
    "        name = fake.company() + \" LTD \" + str(random.randint(10, 99))\n",
    "    generated_company_names.add(name)\n",
    "    return name\n",
    "\n",
    "# Function to generate a unique person name with a two-digit suffix\n",
    "def generate_unique_person_name():\n",
    "    name = fake.name() + \" \" + str(random.randint(10, 99))\n",
    "    while name in generated_person_names:\n",
    "        name = fake.name() + \" \" + str(random.randint(10, 99))\n",
    "    generated_person_names.add(name)\n",
    "    return name\n",
    "\n",
    "# Mask customer_name based on customer_type\n",
    "def mask_customer_name(row):\n",
    "    if row['type'] == 'entities':\n",
    "        return generate_unique_company_name()\n",
    "    elif row['type'] == 'officer':\n",
    "        return generate_unique_person_name()\n",
    "    else:\n",
    "        return row['name']  # Keep the original name if type is not matched\n",
    "\n",
    "# Function to generate a dummy account part\n",
    "def generate_dummy_account_part():\n",
    "    return str(random.randint(1000, 9999))\n",
    "\n",
    "# Mask the last two parts of the account number\n",
    "def mask_account_number(account_number):\n",
    "    parts = account_number.split('-')\n",
    "    if len(parts) > 2:\n",
    "        parts[-2] = generate_dummy_account_part()  # Replace the second-to-last part\n",
    "        parts[-1] = generate_dummy_account_part()  # Replace the last part\n",
    "    return '-'.join(parts)\n",
    "\n",
    "mappings_df['name_masked'] = mappings_df.apply(mask_customer_name, axis=1)\n",
    "mappings_df['account_number_masked'] = mappings_df['account_number'].apply(mask_account_number)\n",
    "\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(mappings_df)\n",
    "\n",
    "# Save the masked mappings DataFrame to a CSV file\n",
    "mappings_df.to_csv('Thesis/masked_mappings.csv', index=False)\n",
    "print(\"Masked mappings DataFrame saved to 'masked_mappings.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGoNkZljKztW"
   },
   "source": [
    "## Join JPMorgan and User (Masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MUkIOUEoK3b_"
   },
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv('Thesis/transactions.csv')\n",
    "\n",
    "# Read the masked_mappings.csv file\n",
    "mappings_df = pd.read_csv('Thesis/masked_mappings.csv')\n",
    "# Join transactions_df with mappings_df on sender_id and Bene_id to account_number\n",
    "sender_mappings_df = mappings_df.add_suffix('_Sender')\n",
    "bene_mappings_df = mappings_df.add_suffix('_Bene')\n",
    "\n",
    "# Perform the joins\n",
    "joined_df = transactions_df.merge(sender_mappings_df, left_on='Sender_Id', right_on='account_number_Sender', how='left')\n",
    "joined_df = joined_df.merge(bene_mappings_df, left_on='Bene_Id', right_on='account_number_Bene', how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "TyvR65UQOTpj",
    "outputId": "c6fc52bc-b071-4924-953f-82a510f59c3f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time_step</th>\n",
       "      <th>Label</th>\n",
       "      <th>Transaction_Id</th>\n",
       "      <th>Sender_Account</th>\n",
       "      <th>Sender_Institution</th>\n",
       "      <th>Sender_Country</th>\n",
       "      <th>USD_amount</th>\n",
       "      <th>Bene_Account</th>\n",
       "      <th>Bene_Institution</th>\n",
       "      <th>Bene_Country</th>\n",
       "      <th>Transaction_Type</th>\n",
       "      <th>is_pep_Sender</th>\n",
       "      <th>customer_id_Sender</th>\n",
       "      <th>name_masked_Sender</th>\n",
       "      <th>account_number_masked_Sender</th>\n",
       "      <th>is_pep_Bene</th>\n",
       "      <th>customer_id_Bene</th>\n",
       "      <th>name_masked_Bene</th>\n",
       "      <th>account_number_masked_Bene</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-07 00:02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>T-174791-02</td>\n",
       "      <td>CHECKING-174582-02</td>\n",
       "      <td>JPMORGANCHASE</td>\n",
       "      <td>United States</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CHECKING-174582-02</td>\n",
       "      <td>JPMORGANCHASE</td>\n",
       "      <td>United States</td>\n",
       "      <td>KYC-ADD-ACCOUNT-OWNER</td>\n",
       "      <td>0.0</td>\n",
       "      <td>191692.0</td>\n",
       "      <td>Tanya Lopez 96</td>\n",
       "      <td>JPMC-CLIENT-4323-9405</td>\n",
       "      <td>0</td>\n",
       "      <td>148629</td>\n",
       "      <td>Christopher Brown 19</td>\n",
       "      <td>CUSTOMER-7684-8182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-07 00:02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>T-105637-03</td>\n",
       "      <td>CHECKING-105426-03</td>\n",
       "      <td>JPMORGANCHASE</td>\n",
       "      <td>United States</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CHECKING-105426-03</td>\n",
       "      <td>JPMORGANCHASE</td>\n",
       "      <td>United States</td>\n",
       "      <td>KYC-ADD-ACCOUNT-OWNER</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177524.0</td>\n",
       "      <td>Amber Stokes 18</td>\n",
       "      <td>JPMC-CLIENT-9768-4398</td>\n",
       "      <td>0</td>\n",
       "      <td>138470</td>\n",
       "      <td>Ryan White 87</td>\n",
       "      <td>CUSTOMER-4405-5677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-24 00:51:00</td>\n",
       "      <td>0</td>\n",
       "      <td>T-105642-03</td>\n",
       "      <td>CHECKING-105425-03</td>\n",
       "      <td>JPMORGANCHASE</td>\n",
       "      <td>United States</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CHECKING-105425-03</td>\n",
       "      <td>JPMORGANCHASE</td>\n",
       "      <td>United States</td>\n",
       "      <td>KYC-ADD-ACCOUNT-OWNER</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177524.0</td>\n",
       "      <td>Amber Stokes 18</td>\n",
       "      <td>JPMC-CLIENT-9768-4398</td>\n",
       "      <td>0</td>\n",
       "      <td>138470</td>\n",
       "      <td>Ryan White 87</td>\n",
       "      <td>CUSTOMER-4405-5677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-07 00:02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>T-235858-04</td>\n",
       "      <td>CHECKING-235577-04</td>\n",
       "      <td>JPMORGANCHASE</td>\n",
       "      <td>United States</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CHECKING-235577-04</td>\n",
       "      <td>JPMORGANCHASE</td>\n",
       "      <td>United States</td>\n",
       "      <td>KYC-ADD-ACCOUNT-OWNER</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608133.0</td>\n",
       "      <td>Mckinney-Hanna LTD 15</td>\n",
       "      <td>COMPANY-1419-6653</td>\n",
       "      <td>0</td>\n",
       "      <td>199857</td>\n",
       "      <td>Parker Rush 37</td>\n",
       "      <td>JPMC-CLIENT-3264-6924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-07 00:02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>T-235858-04</td>\n",
       "      <td>CHECKING-235577-04</td>\n",
       "      <td>JPMORGANCHASE</td>\n",
       "      <td>United States</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CHECKING-235577-04</td>\n",
       "      <td>JPMORGANCHASE</td>\n",
       "      <td>United States</td>\n",
       "      <td>KYC-ADD-ACCOUNT-OWNER</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608133.0</td>\n",
       "      <td>Mckinney-Hanna LTD 15</td>\n",
       "      <td>COMPANY-1419-6653</td>\n",
       "      <td>0</td>\n",
       "      <td>981288</td>\n",
       "      <td>Jonathan Young 58</td>\n",
       "      <td>JPMC-CLIENT-1430-7697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Time_step  Label Transaction_Id      Sender_Account  \\\n",
       "0  2022-01-07 00:02:00      0    T-174791-02  CHECKING-174582-02   \n",
       "1  2022-01-07 00:02:00      0    T-105637-03  CHECKING-105426-03   \n",
       "2  2022-07-24 00:51:00      0    T-105642-03  CHECKING-105425-03   \n",
       "3  2022-01-07 00:02:00      0    T-235858-04  CHECKING-235577-04   \n",
       "4  2022-01-07 00:02:00      0    T-235858-04  CHECKING-235577-04   \n",
       "\n",
       "  Sender_Institution Sender_Country  USD_amount        Bene_Account  \\\n",
       "0      JPMORGANCHASE  United States         0.0  CHECKING-174582-02   \n",
       "1      JPMORGANCHASE  United States         0.0  CHECKING-105426-03   \n",
       "2      JPMORGANCHASE  United States         0.0  CHECKING-105425-03   \n",
       "3      JPMORGANCHASE  United States         0.0  CHECKING-235577-04   \n",
       "4      JPMORGANCHASE  United States         0.0  CHECKING-235577-04   \n",
       "\n",
       "  Bene_Institution   Bene_Country       Transaction_Type  is_pep_Sender  \\\n",
       "0    JPMORGANCHASE  United States  KYC-ADD-ACCOUNT-OWNER            0.0   \n",
       "1    JPMORGANCHASE  United States  KYC-ADD-ACCOUNT-OWNER            0.0   \n",
       "2    JPMORGANCHASE  United States  KYC-ADD-ACCOUNT-OWNER            0.0   \n",
       "3    JPMORGANCHASE  United States  KYC-ADD-ACCOUNT-OWNER            0.0   \n",
       "4    JPMORGANCHASE  United States  KYC-ADD-ACCOUNT-OWNER            0.0   \n",
       "\n",
       "   customer_id_Sender     name_masked_Sender account_number_masked_Sender  \\\n",
       "0            191692.0         Tanya Lopez 96        JPMC-CLIENT-4323-9405   \n",
       "1            177524.0        Amber Stokes 18        JPMC-CLIENT-9768-4398   \n",
       "2            177524.0        Amber Stokes 18        JPMC-CLIENT-9768-4398   \n",
       "3            608133.0  Mckinney-Hanna LTD 15            COMPANY-1419-6653   \n",
       "4            608133.0  Mckinney-Hanna LTD 15            COMPANY-1419-6653   \n",
       "\n",
       "   is_pep_Bene  customer_id_Bene      name_masked_Bene  \\\n",
       "0            0            148629  Christopher Brown 19   \n",
       "1            0            138470         Ryan White 87   \n",
       "2            0            138470         Ryan White 87   \n",
       "3            0            199857        Parker Rush 37   \n",
       "4            0            981288     Jonathan Young 58   \n",
       "\n",
       "  account_number_masked_Bene  \n",
       "0         CUSTOMER-7684-8182  \n",
       "1         CUSTOMER-4405-5677  \n",
       "2         CUSTOMER-4405-5677  \n",
       "3      JPMC-CLIENT-3264-6924  \n",
       "4      JPMC-CLIENT-1430-7697  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = [\n",
    "    'Bene_Id', 'node_id_Sender', 'type_Sender', 'country_codes_Sender', 'countries_Sender','account_number_Sender','name_Sender',\n",
    "    'Sender_Id', 'node_id_Bene', 'type_Bene', 'country_codes_Bene', 'countries_Bene','account_number_Bene','name_Bene'\n",
    "]\n",
    "joined_df = joined_df.drop(columns=columns_to_drop)\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LyqciToUQP7j",
    "outputId": "cb2fc4f8-747a-4f7b-af83-7f29907dc77b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1262263 entries, 0 to 1262262\n",
      "Data columns (total 19 columns):\n",
      " #   Column                        Non-Null Count    Dtype  \n",
      "---  ------                        --------------    -----  \n",
      " 0   Time_step                     1262263 non-null  object \n",
      " 1   Label                         1262263 non-null  int64  \n",
      " 2   Transaction_Id                1262263 non-null  object \n",
      " 3   Sender_Account                748161 non-null   object \n",
      " 4   Sender_Institution            748161 non-null   object \n",
      " 5   Sender_Country                748161 non-null   object \n",
      " 6   USD_Amount                    1262263 non-null  float64\n",
      " 7   Bene_Account                  1262263 non-null  object \n",
      " 8   Bene_Institution              1262263 non-null  object \n",
      " 9   Bene_Country                  1262263 non-null  object \n",
      " 10  Transaction_Type              1262263 non-null  object \n",
      " 11  Sender_Is_Pep                 1262263 non-null  int64  \n",
      " 12  Sender_Customer_Id            1262263 non-null  int64  \n",
      " 13  Sender_Name_Masked            748161 non-null   object \n",
      " 14  Sender_Account_Number_Masked  748161 non-null   object \n",
      " 15  Bene_Is_Pep                   1262263 non-null  int64  \n",
      " 16  Bene_Customer_Id              1262263 non-null  int64  \n",
      " 17  Bene_Name_Masked              1262263 non-null  object \n",
      " 18  Bene_Account_Number_Masked    1262263 non-null  object \n",
      "dtypes: float64(1), int64(5), object(13)\n",
      "memory usage: 192.6+ MB\n"
     ]
    }
   ],
   "source": [
    "joined_df['is_pep_Sender'] = joined_df['is_pep_Sender'].fillna(0).astype(int)\n",
    "joined_df['customer_id_Sender'] = joined_df['customer_id_Sender'].fillna(0).astype(int)\n",
    "\n",
    "joined_df['is_pep_Bene'] = joined_df['is_pep_Bene'].fillna(0).astype(int)\n",
    "joined_df['customer_id_Bene'] = joined_df['customer_id_Bene'].fillna(0).astype(int)\n",
    "\n",
    "joined_df = joined_df.rename(columns={'is_pep_Sender': 'Sender_Is_Pep', 'customer_id_Sender': 'Sender_Customer_Id',\n",
    "                                      'is_pep_Bene':'Bene_Is_Pep','customer_id_Bene':'Bene_Customer_Id',\n",
    "                                      'name_masked_Sender':'Sender_Name_Masked','USD_amount':'USD_Amount',\n",
    "                                      'account_number_masked_Sender':'Sender_Account_Number_Masked','name_masked_Bene':'Bene_Name_Masked',\n",
    "                                      'account_number_masked_Bene':'Bene_Account_Number_Masked'})\n",
    "joined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DLLnk5bzSrj2"
   },
   "outputs": [],
   "source": [
    "joined_df.to_csv('Thesis/transactions_all.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaTxpWB4UevW"
   },
   "source": [
    "## Split Train Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGUOzuRmU0Ym",
    "outputId": "e7bf7307-4ae7-4485-a52b-7a27a31aaedd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The date at the 70% split point is: 2023-05-03 20:41:00\n",
      "Class distribution in the train set:\n",
      "0    632532\n",
      "1    251052\n",
      "Name: Label, dtype: int64\n",
      "Class distribution in the test set:\n",
      "0    195666\n",
      "1    183013\n",
      "Name: Label, dtype: int64\n",
      "Train and test sets saved to 'Thesis/' directory.\n"
     ]
    }
   ],
   "source": [
    "transactions_df = pd.read_csv('Thesis/transactions_all.csv')\n",
    "\n",
    "# Convert the date column to datetime if it isn't already\n",
    "transactions_df['Time_step'] = pd.to_datetime(transactions_df['Time_step'])\n",
    "\n",
    "# Sort the DataFrame by the Time_step column to ensure chronological order\n",
    "transactions_df = transactions_df.sort_values(by='Time_step')\n",
    "\n",
    "# Calculate the 70% split point based on the number of rows\n",
    "split_index = int(len(transactions_df) * 0.7)\n",
    "\n",
    "# Print the date at the split index\n",
    "split_date = transactions_df.iloc[split_index]['Time_step']\n",
    "print(f\"The date at the 70% split point is: {split_date}\")\n",
    "\n",
    "# Split the data into train and test sets based on the calculated split index\n",
    "train_df = transactions_df.iloc[:split_index]\n",
    "test_df = transactions_df.iloc[split_index:]\n",
    "\n",
    "# Print the class distribution in the train and test sets\n",
    "print(\"Class distribution in the train set:\")\n",
    "print(train_df['Label'].value_counts())\n",
    "print(\"Class distribution in the test set:\")\n",
    "print(test_dfin['Label'].value_counts())\n",
    "\n",
    "# Save the train and test sets to CSV files\n",
    "train_df.to_csv('Thesis/train.csv', index=False)\n",
    "test_df.to_csv('Thesis/test.csv', index=False)\n",
    "print(\"Train and test sets saved to 'Thesis/' directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sM1BMXkdViSw"
   },
   "source": [
    "## Fuzzy Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yq7rPo00V_Pj",
    "outputId": "889b3265-273b-4dc8-c0b2-40ea3a0a6b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: org.freedesktop.DBus.Error.FileNotFound: Failed to connect to socket /run/user/16444/bus: No such file or directory\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-fuzzy\n",
      "  Using cached scikit-fuzzy-0.4.2.tar.gz (993 kB)\n",
      "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from scikit-fuzzy) (1.19.4)\n",
      "Requirement already satisfied: scipy>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from scikit-fuzzy) (1.5.4)\n",
      "Collecting networkx>=1.9.0\n",
      "  Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.9.0->scikit-fuzzy) (4.4.2)\n",
      "Building wheels for collected packages: scikit-fuzzy\n",
      "  Building wheel for scikit-fuzzy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scikit-fuzzy: filename=scikit_fuzzy-0.4.2-py3-none-any.whl size=894069 sha256=27377fb16f58f458f645c07b9f72f852ca7df137c9e4aafa9f8d7bb6a5693cdc\n",
      "  Stored in directory: /home/echristi/.cache/pip/wheels/31/1e/58/db8cfe08f81c72d8c31bc58690ce63d9e3d93a6e97dca5ddb4\n",
      "Successfully built scikit-fuzzy\n",
      "Installing collected packages: networkx, scikit-fuzzy\n",
      "Successfully installed networkx-2.5.1 scikit-fuzzy-0.4.2\n",
      "\u001b[33mWARNING: You are using pip version 20.3.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikit-fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e_tSgGtWHTw",
    "outputId": "938546f6-a743-45e7-fcee-dd23db497055"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skfuzzy as fuzz\n",
    "import os\n",
    "from skfuzzy import control as ctrl\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import os\n",
    "amount = ctrl.Antecedent(np.arange(0, 10000, 1000), 'amount')\n",
    "cross_border = ctrl.Antecedent(np.arange(0, 2, 1), 'cross_border')\n",
    "country_risk = ctrl.Antecedent(np.arange(0, 2, 1), 'country_risk')  # For country risk like Iran, Syria, North Korea\n",
    "risk = ctrl.Consequent(np.arange(0, 101, 1), 'risk')\n",
    "\n",
    "# Membership Functions\n",
    "amount.automf(3, names=['low', 'medium', 'high'])\n",
    "cross_border.automf(3, names=['domestic', 'mixed', 'international'])\n",
    "country_risk['low'] = fuzz.trimf(country_risk.universe, [0, 0, 0.5])\n",
    "country_risk['high'] = fuzz.trimf(country_risk.universe, [0.5, 1, 1])\n",
    "\n",
    "risk['low'] = fuzz.trimf(risk.universe, [0, 0, 50])\n",
    "risk['medium'] = fuzz.trimf(risk.universe, [20, 50, 80])\n",
    "risk['high'] = fuzz.trimf(risk.universe, [60, 100, 100])\n",
    "\n",
    "# Fuzzy rules\n",
    "rule1 = ctrl.Rule(amount['high'] | cross_border['international'], risk['high'])\n",
    "rule2 = ctrl.Rule(amount['medium'] & cross_border['domestic'], risk['medium'])\n",
    "rule3 = ctrl.Rule(amount['low'] & (cross_border['domestic'] | cross_border['mixed']), risk['low'])\n",
    "rule4 = ctrl.Rule(country_risk['high'], risk['high'])\n",
    "\n",
    "# Control system setup\n",
    "aml_control = ctrl.ControlSystem([rule1, rule2, rule3, rule4])\n",
    "aml_sim = ctrl.ControlSystemSimulation(aml_control)\n",
    "\n",
    "# Function to apply fuzzy system to each transaction\n",
    "def evaluate_transaction(row):\n",
    "    reasons = []\n",
    "\n",
    "\n",
    "    aml_sim.inputs({\n",
    "        'amount': row['USD_Amount'],\n",
    "        'cross_border': 1 if row['Sender_Country'] != row['Bene_Country'] else 0,\n",
    "        'country_risk': 1 if row['Bene_Country'] in ['Iran', 'Syria', 'North-Korea'] else 0\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        # Compute risk score\n",
    "        aml_sim.compute()\n",
    "        risk_score = aml_sim.output['risk']\n",
    "        if risk_score >= 60:  # Assuming 60 as the threshold for high risk\n",
    "            if row['USD_Amount'] >= amount.universe.max():\n",
    "                reasons.append(\"High Amount\")\n",
    "            if 1 if row['Sender_Country'] != row['Bene_Country'] else 0 == 1:\n",
    "                reasons.append(\"Cross Border Transaction\")\n",
    "            if 1 if row['Bene_Country'] in ['Iran', 'Syria', 'North-Korea'] else 0 == 1:\n",
    "                reasons.append(\"High Risk Country\")\n",
    "        return risk_score, reasons\n",
    "    except Exception as e:\n",
    "        print(f\"Error during risk evaluation: {e}\")\n",
    "        return np.nan, reasons  # Return NaN if an error occurs (no rules fired)\n",
    "\n",
    "\n",
    "\n",
    "train_df['risk_score'], train_df['fuzzy_result'] = zip(*train_df.apply(evaluate_transaction, axis=1))\n",
    "train_df['fuzzy_result'] = train_df['fuzzy_result'].apply(\n",
    "    lambda x: 'None' if isinstance(x, list) and not x else x if isinstance(x, list) else str(x)\n",
    ")\n",
    "test_df['risk_score'], test_df['fuzzy_result'] = zip(*test_df.apply(evaluate_transaction, axis=1))\n",
    "test_df['fuzzy_result'] = test_df['fuzzy_result'].apply(\n",
    "    lambda x: 'None' if isinstance(x, list) and not x else x if isinstance(x, list) else str(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ti8e7zKAZBVU"
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('Thesis/train_with_fuzzy_results.csv', index=False)\n",
    "test_df.to_csv('Thesis/test_with_fuzzy_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GClR9OpDYKvl"
   },
   "source": [
    "## Testing 1 GCN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efq3SkAUbNWR",
    "outputId": "5f1f3892-4fcd-46af-d77c-9a2c25608a12"
   },
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install torch-geometric\n",
    "# !pip install torch-sparse\n",
    "# !pip install torch-scatter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: org.freedesktop.DBus.Error.FileNotFound: Failed to connect to socket /run/user/16444/bus: No such file or directory\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting optuna\n",
      "  Downloading optuna-3.0.6-py3-none-any.whl (348 kB)\n",
      "\u001b[K     |################################| 348 kB 759 bytes/s  0:00:01\n",
      "\u001b[?25hCollecting cliff\n",
      "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
      "\u001b[K     |################################| 81 kB 14 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (20.8)\n",
      "Collecting sqlalchemy>=1.3.0\n",
      "  Downloading SQLAlchemy-1.4.53-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |################################| 1.6 MB 10 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.54.1)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\n",
      "\u001b[K     |################################| 210 kB 9.6 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.19.4)\n",
      "Requirement already satisfied: scipy!=1.4.0,<1.9.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.5.4)\n",
      "Collecting colorlog\n",
      "  Using cached colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Collecting cmaes>=0.8.2\n",
      "  Downloading cmaes-0.9.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata<5.0.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (3.3.0)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from optuna) (3.12)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.0.7)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata<5.0.0->optuna) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata<5.0.0->optuna) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->optuna) (2.4.7)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/greenlet/\u001b[0m\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-2.0.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (564 kB)\n",
      "\u001b[K     |################################| 564 kB 1.9 kB/s s eta 0:00:01\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/stevedore/\u001b[0m\n",
      "\u001b[?25hCollecting stevedore>=2.0.1\n",
      "  Downloading stevedore-3.5.2-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |################################| 50 kB 10 kB/s s eta 0:00:01\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/prettytable/\u001b[0m\n",
      "\u001b[?25hCollecting PrettyTable>=0.7.2\n",
      "  Downloading prettytable-2.5.0-py3-none-any.whl (24 kB)\n",
      "Collecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-6.0.0-py2.py3-none-any.whl (107 kB)\n",
      "\u001b[K     |################################| 107 kB 84.2 MB/s eta 0:00:01\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/autopage/\u001b[0m\n",
      "\u001b[?25hCollecting autopage>=0.4.0\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/9b/63/f1c3fa431e91a52bad5e3602e9d5df6c94d8d095ac485424efa4eeddb4d2/autopage-0.5.2-py3-none-any.whl\u001b[0m\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', OSError(107, 'Transport endpoint is not connected'))': /packages/9b/63/f1c3fa431e91a52bad5e3602e9d5df6c94d8d095ac485424efa4eeddb4d2/autopage-0.5.2-py3-none-any.whl\u001b[0m\n",
      "  Downloading autopage-0.5.2-py3-none-any.whl (30 kB)\n",
      "Collecting cmd2>=1.0.0\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/f3/9a/495a0577cbae4a11dc0b2a2174688f0bab83d1b81245a105f1613a365828/cmd2-2.4.3-py3-none-any.whl\u001b[0m\n",
      "  Downloading cmd2-2.4.3-py3-none-any.whl (147 kB)\n",
      "\u001b[K     |################################| 147 kB 29.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyperclip>=1.6\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/30/23/2f0a3efc4d6a32f3b63cdff36cd398d9701d26cda58e3ab97ac79fb5e60d/pyperclip-1.9.0.tar.gz\u001b[0m\n",
      "  Downloading pyperclip-1.9.0.tar.gz (20 kB)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2>=1.0.0->cliff->optuna) (20.3.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Building wheels for collected packages: pyperclip\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyperclip: filename=pyperclip-1.9.0-py3-none-any.whl size=10987 sha256=aa2d3a440f98e5e2c441c71a2c676350b4e27f6d8cd29edb0610484defeba703\n",
      "  Stored in directory: /home/echristi/.cache/pip/wheels/76/f7/a1/7f6745af2b6d9ebe4f0fc6dfb97f2514afc3d8fc8e08dc9aa1\n",
      "Successfully built pyperclip\n",
      "Installing collected packages: pyperclip, pbr, greenlet, stevedore, sqlalchemy, PrettyTable, importlib-resources, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
      "\u001b[33m  WARNING: The script pbr is installed in '/home/echristi/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pac5FrW9YTB7",
    "outputId": "fa8d9a1a-b6b6-4f81-95db-e4a34d978106"
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class EdgeGCN_LSTM(nn.Module):\n",
    "    def __init__(self, hidden_channels, lstm_hidden_channels, out_channels, dropout_rate):\n",
    "        super(EdgeGCN_LSTM, self).__init__()\n",
    "        self.conv1 = GCNConv(1, hidden_channels)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.lstm = nn.LSTM(input_size=hidden_channels * 2 + 4, hidden_size=lstm_hidden_channels, batch_first=True)\n",
    "        self.lin = nn.Linear(lstm_hidden_channels, out_channels)\n",
    "        self.dropout_rate = dropout_rate\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = F.dropout(F.relu(self.bn1(self.conv1(x, edge_index))), p=self.dropout_rate, training=self.training)\n",
    "        x = F.dropout(F.relu(self.bn2(self.conv2(x, edge_index))), p=self.dropout_rate, training=self.training)\n",
    "        sender_features = x[edge_index[0]]\n",
    "        receiver_features = x[edge_index[1]]\n",
    "        edge_features = torch.cat([sender_features, receiver_features, edge_attr], dim=1)\n",
    "        edge_features = edge_features.unsqueeze(0)  # Add batch dimension for LSTM\n",
    "        lstm_out, _ = self.lstm(edge_features)\n",
    "        lstm_out = lstm_out.squeeze(0)  # Remove batch dimension\n",
    "        out = self.lin(lstm_out)\n",
    "        return out.view(-1)\n",
    "\n",
    "class GraphDataProcessor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def undersample_df(self):\n",
    "        fraud_df = self.df[self.df['Label'] == 1]\n",
    "        non_fraud_df = self.df[self.df['Label'] == 0]\n",
    "        print(f\"Initial fraud cases: {len(fraud_df)}, non-fraud cases: {len(non_fraud_df)}\")\n",
    "        balanced_df = non_fraud_df.sample(len(fraud_df), random_state=42)\n",
    "        self.df = pd.concat([fraud_df, balanced_df])\n",
    "        print(f\"Balanced dataset: {len(self.df)} records\")\n",
    "\n",
    "    def prepare_graph_data(self):\n",
    "        self.undersample_df()\n",
    "        self.df['Time_step'] = pd.to_datetime(self.df['Time_step'])\n",
    "        self.df = self.df.sort_values(by=['Sender_Customer_Id', 'Time_step'])\n",
    "        self.df['Label'] = pd.to_numeric(self.df['Label'], errors='coerce').fillna(0).astype(int)\n",
    "        self.df['Days_Since_Last'] = self.df.groupby('Sender_Customer_Id')['Time_step'].diff().dt.days.fillna(0)\n",
    "\n",
    "        all_ids = pd.concat([self.df['Sender_Customer_Id'], self.df['Bene_Customer_Id']]).unique()\n",
    "        if len(all_ids) == 0:\n",
    "            raise ValueError(\"No unique IDs found in the dataset\")\n",
    "\n",
    "        id_map = {id: idx for idx, id in enumerate(all_ids)}\n",
    "        edge_index = torch.tensor([self.df['Sender_Customer_Id'].map(id_map).values, self.df['Bene_Customer_Id'].map(id_map).values], dtype=torch.long)\n",
    "\n",
    "        node_features = torch.zeros((len(all_ids), 1))\n",
    "        if self.df['Transaction_Type'].isnull().any() or self.df['USD_Amount'].isnull().any() or self.df['risk_score'].isnull().any():\n",
    "            raise ValueError(\"Null values found in essential columns\")\n",
    "\n",
    "        transaction_type_encoded = torch.tensor(LabelEncoder().fit_transform(self.df['Transaction_Type']), dtype=torch.float).view(-1, 1)\n",
    "        usd_amount = torch.tensor(StandardScaler().fit_transform(self.df[['USD_Amount']]), dtype=torch.float).view(-1, 1)\n",
    "        risk_score = torch.tensor(self.df['risk_score'].values, dtype=torch.float).view(-1, 1)\n",
    "        days_since_last = torch.tensor(StandardScaler().fit_transform(self.df[['Days_Since_Last']]), dtype=torch.float).view(-1, 1)\n",
    "\n",
    "        edge_attr = torch.cat([transaction_type_encoded, usd_amount, risk_score, days_since_last], dim=1)\n",
    "        edge_labels = torch.tensor(self.df['Label'].values, dtype=torch.long)\n",
    "\n",
    "        return Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=edge_labels)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_val_df, test_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df['Label']\n",
    ")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=train_val_df['Label']\n",
    ")\n",
    "\n",
    "train_processor = GraphDataProcessor(train_df)\n",
    "val_processor = GraphDataProcessor(val_df)\n",
    "test_processor = GraphDataProcessor(test_df)\n",
    "\n",
    "train_data = train_processor.prepare_graph_data()\n",
    "val_data = val_processor.prepare_graph_data()\n",
    "test_data = test_processor.prepare_graph_data()\n",
    "\n",
    "train_loader = DataLoader([train_data], batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader([val_data], batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader([test_data], batch_size=32, shuffle=False)\n",
    "\n",
    "def train(model, device, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.edge_attr)\n",
    "        loss = criterion(output, data.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, device, loader, criterion):\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_scores = [], [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data.x, data.edge_index, data.edge_attr)\n",
    "            loss = criterion(output, data.y.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.sigmoid(output).cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "\n",
    "            y_scores.extend(probs)\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(data.y.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    ks_statistic = max(tpr - fpr)\n",
    "\n",
    "    return total_loss / len(loader), f1, precision, recall, auc, ks_statistic\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    hidden_channels = trial.suggest_categorical('hidden_channels', [16, 32, 64])\n",
    "    lstm_hidden_channels = trial.suggest_categorical('lstm_hidden_channels', [16, 32, 64])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.7)\n",
    "\n",
    "    model = EdgeGCN_LSTM(hidden_channels=hidden_channels, lstm_hidden_channels=lstm_hidden_channels, out_channels=1, dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_model_path = \"gcn_lstm.pth\"\n",
    "    for epoch in range(10):\n",
    "        train_loss = train(model, device, train_loader, optimizer, criterion)\n",
    "        val_loss, f1, precision, recall, auc, ks_statistic = evaluate(model, device, val_loader, criterion)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            # Save both model state and hyperparameters\n",
    "            checkpoint = {\n",
    "                'state_dict': model.state_dict(),\n",
    "                'hyperparameters': {\n",
    "                    'hidden_channels': hidden_channels,\n",
    "                    'lstm_hidden_channels': lstm_hidden_channels,\n",
    "                    'out_channels': 1,\n",
    "                    'dropout_rate': dropout_rate\n",
    "                },\n",
    "                'metrics': {\n",
    "                    'f1': f1,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'auc': auc,\n",
    "                    'ks_statistic': ks_statistic\n",
    "                }\n",
    "            }\n",
    "            torch.save(checkpoint, best_model_path)\n",
    "            shutil.copy(best_model_path, f\"/content/drive/My Drive/{best_model_path}\")\n",
    "\n",
    "    return best_f1  # Optimize for the best F1 score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\" Value (F1 Score): {trial.value}\")\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Load the best model to print all metrics\n",
    "checkpoint = torch.load(f\"/content/drive/My Drive/gcn_lstm.pth\")\n",
    "metrics = checkpoint['metrics']\n",
    "print(\" Validation set metrics:\")\n",
    "print(f\"    F1 Score: {metrics['f1']}\")\n",
    "print(f\"    Precision: {metrics['precision']}\")\n",
    "print(f\"    Recall: {metrics['recall']}\")\n",
    "print(f\"    AUC: {metrics['auc']}\")\n",
    "print(f\"    KS Statistic: {metrics['ks_statistic']}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model = EdgeGCN_LSTM(\n",
    "    hidden_channels=checkpoint['hyperparameters']['hidden_channels'],\n",
    "    lstm_hidden_channels=checkpoint['hyperparameters']['lstm_hidden_channels'],\n",
    "    out_channels=1,\n",
    "    dropout_rate=checkpoint['hyperparameters']['dropout_rate']\n",
    ").to(device)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "test_loss, test_f1, test_precision, test_recall, test_auc, test_ks_statistic = evaluate(model, device, test_loader, criterion)\n",
    "\n",
    "print(\" Test set metrics:\")\n",
    "print(f\"    Loss: {test_loss}\")\n",
    "print(f\"    F1 Score: {test_f1}\")\n",
    "print(f\"    Precision: {test_precision}\")\n",
    "print(f\"    Recall: {test_recall}\")\n",
    "print(f\"    AUC: {test_auc}\")\n",
    "print(f\"    KS Statistic: {test_ks_statistic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dypmtfTYf58s"
   },
   "source": [
    "## Testing 2 GCN+GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OO-5dOBjMiSh"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/content/drive/My Drive/Thesis/train_with_fuzzy_results.csv')\n",
    "test_df = pd.read_csv('/content/drive/My Drive/Thesis/test_with_fuzzy_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8m6jEifSf-2O",
    "outputId": "d4fcff60-2bce-481b-c0d0-baa8967e29f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial fraud cases: 53343, non-fraud cases: 203323\n",
      "Balanced dataset: 106686 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-0ff8dbff1a36>:39: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  edge_index = torch.tensor([self.df['Sender_Customer_Id'].map(id_map).values, self.df['Bene_Customer_Id'].map(id_map).values], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial fraud cases: 17781, non-fraud cases: 67775\n",
      "Balanced dataset: 35562 records\n",
      "Initial fraud cases: 17781, non-fraud cases: 67775\n",
      "Balanced dataset: 35562 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "[I 2024-08-06 10:19:26,767] A new study created in memory with name: no-name-02d53bf9-977a-4234-a44f-4930786e05ce\n",
      "<ipython-input-9-0ff8dbff1a36>:117: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
      "<ipython-input-9-0ff8dbff1a36>:120: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.7)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-08-06 10:25:43,160] Trial 0 finished with value: 0.7127356489058617 and parameters: {'lr': 0.03182292588617258, 'hidden_channels': 32, 'gru_hidden_channels': 64, 'dropout_rate': 0.12964183950169697}. Best is trial 0 with value: 0.7127356489058617.\n",
      "<ipython-input-9-0ff8dbff1a36>:117: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
      "<ipython-input-9-0ff8dbff1a36>:120: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.7)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-08-06 10:32:01,929] Trial 1 finished with value: 0.7035024479474902 and parameters: {'lr': 0.042479005419784444, 'hidden_channels': 64, 'gru_hidden_channels': 64, 'dropout_rate': 0.4742345879102148}. Best is trial 0 with value: 0.7127356489058617.\n",
      "<ipython-input-9-0ff8dbff1a36>:117: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
      "<ipython-input-9-0ff8dbff1a36>:120: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.7)\n",
      "[I 2024-08-06 10:38:05,870] Trial 2 finished with value: 0.6683958274598252 and parameters: {'lr': 6.961643175380326e-05, 'hidden_channels': 16, 'gru_hidden_channels': 32, 'dropout_rate': 0.2895093187146906}. Best is trial 0 with value: 0.7127356489058617.\n",
      "<ipython-input-9-0ff8dbff1a36>:117: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
      "<ipython-input-9-0ff8dbff1a36>:120: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.7)\n",
      "[I 2024-08-06 10:44:24,654] Trial 3 finished with value: 0.8039279869067103 and parameters: {'lr': 0.01289683418099001, 'hidden_channels': 64, 'gru_hidden_channels': 64, 'dropout_rate': 0.5888929333646283}. Best is trial 3 with value: 0.8039279869067103.\n",
      "<ipython-input-9-0ff8dbff1a36>:117: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
      "<ipython-input-9-0ff8dbff1a36>:120: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.7)\n",
      "[I 2024-08-06 10:50:16,420] Trial 4 finished with value: 0.41402689366831147 and parameters: {'lr': 0.00044863467449097517, 'hidden_channels': 64, 'gru_hidden_channels': 16, 'dropout_rate': 0.36093863759502154}. Best is trial 3 with value: 0.8039279869067103.\n",
      "<ipython-input-9-0ff8dbff1a36>:117: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
      "<ipython-input-9-0ff8dbff1a36>:120: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.7)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-08-06 10:56:00,454] Trial 5 finished with value: 0.7339625659894419 and parameters: {'lr': 0.0033110428456065667, 'hidden_channels': 16, 'gru_hidden_channels': 32, 'dropout_rate': 0.6612869898069086}. Best is trial 3 with value: 0.8039279869067103.\n",
      "<ipython-input-9-0ff8dbff1a36>:117: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
      "<ipython-input-9-0ff8dbff1a36>:120: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.7)\n",
      "[I 2024-08-06 11:01:58,061] Trial 6 finished with value: 0.6671054116054304 and parameters: {'lr': 0.0038150973230998394, 'hidden_channels': 16, 'gru_hidden_channels': 16, 'dropout_rate': 0.599800749428779}. Best is trial 3 with value: 0.8039279869067103.\n",
      "<ipython-input-9-0ff8dbff1a36>:117: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
      "<ipython-input-9-0ff8dbff1a36>:120: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.7)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import optuna\n",
    "import shutil\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class GraphDataProcessor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def undersample_df(self):\n",
    "        fraud_df = self.df[self.df['Label'] == 1]\n",
    "        non_fraud_df = self.df[self.df['Label'] == 0]\n",
    "        print(f\"Initial fraud cases: {len(fraud_df)}, non-fraud cases: {len(non_fraud_df)}\")\n",
    "        balanced_df = non_fraud_df.sample(len(fraud_df), random_state=42)\n",
    "        self.df = pd.concat([fraud_df, balanced_df])\n",
    "        print(f\"Balanced dataset: {len(self.df)} records\")\n",
    "\n",
    "    def prepare_graph_data(self):\n",
    "        self.undersample_df()\n",
    "        self.df['Time_step'] = pd.to_datetime(self.df['Time_step'])\n",
    "        self.df = self.df.sort_values(by=['Sender_Customer_Id', 'Time_step'])\n",
    "        self.df['Label'] = pd.to_numeric(self.df['Label'], errors='coerce').fillna(0).astype(int)\n",
    "        self.df['Days_Since_Last'] = self.df.groupby('Sender_Customer_Id')['Time_step'].diff().dt.days.fillna(0)\n",
    "\n",
    "        all_ids = pd.concat([self.df['Sender_Customer_Id'], self.df['Bene_Customer_Id']]).unique()\n",
    "        if len(all_ids) == 0:\n",
    "            raise ValueError(\"No unique IDs found in the dataset\")\n",
    "\n",
    "        id_map = {id: idx for idx, id in enumerate(all_ids)}\n",
    "        edge_index = torch.tensor([self.df['Sender_Customer_Id'].map(id_map).values, self.df['Bene_Customer_Id'].map(id_map).values], dtype=torch.long)\n",
    "\n",
    "        node_features = torch.zeros((len(all_ids), 1))\n",
    "        if self.df['Transaction_Type'].isnull().any() or self.df['USD_Amount'].isnull().any() or self.df['risk_score'].isnull().any():\n",
    "            raise ValueError(\"Null values found in essential columns\")\n",
    "\n",
    "        transaction_type_encoded = torch.tensor(LabelEncoder().fit_transform(self.df['Transaction_Type']), dtype=torch.float).view(-1, 1)\n",
    "        usd_amount = torch.tensor(StandardScaler().fit_transform(self.df[['USD_Amount']]), dtype=torch.float).view(-1, 1)\n",
    "        risk_score = torch.tensor(self.df['risk_score'].values, dtype=torch.float).view(-1, 1)\n",
    "        days_since_last = torch.tensor(StandardScaler().fit_transform(self.df[['Days_Since_Last']]), dtype=torch.float).view(-1, 1)\n",
    "\n",
    "        edge_attr = torch.cat([transaction_type_encoded, usd_amount, risk_score, days_since_last], dim=1)\n",
    "        edge_labels = torch.tensor(self.df['Label'].values, dtype=torch.long)\n",
    "\n",
    "        return Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=edge_labels)\n",
    "\n",
    "class EdgeGCN_GRU(nn.Module):\n",
    "    def __init__(self, hidden_channels, gru_hidden_channels, out_channels, dropout_rate):\n",
    "        super(EdgeGCN_GRU, self).__init__()\n",
    "        self.conv1 = GCNConv(1, hidden_channels)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.gru = nn.GRU(input_size=hidden_channels * 2 + 4, hidden_size=gru_hidden_channels, batch_first=True)\n",
    "        self.lin = nn.Linear(gru_hidden_channels, out_channels)\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = F.dropout(F.relu(self.bn1(self.conv1(x, edge_index))), p=self.dropout_rate, training=self.training)\n",
    "        x = F.dropout(F.relu(self.bn2(self.conv2(x, edge_index))), p=self.dropout_rate, training=self.training)\n",
    "        sender_features = x[edge_index[0]]\n",
    "        receiver_features = x[edge_index[1]]\n",
    "        edge_features = torch.cat([sender_features, receiver_features, edge_attr], dim=1)\n",
    "        edge_features = edge_features.unsqueeze(0)  # Add batch dimension for GRU\n",
    "        gru_out, _ = self.gru(edge_features)\n",
    "        gru_out = gru_out.squeeze(0)  # Remove batch dimension\n",
    "        out = self.lin(gru_out)\n",
    "        return out.view(-1)\n",
    "def train(model, device, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.edge_attr)\n",
    "        loss = criterion(output, data.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, device, loader, criterion):\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_scores = [], [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data.x, data.edge_index, data.edge_attr)\n",
    "            loss = criterion(output, data.y.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.sigmoid(output).cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "\n",
    "            y_scores.extend(probs)\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(data.y.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    ks_statistic = max(tpr - fpr)\n",
    "\n",
    "    return total_loss / len(loader), f1, precision, recall, auc, ks_statistic\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    hidden_channels = trial.suggest_categorical('hidden_channels', [16, 32, 64])\n",
    "    gru_hidden_channels = trial.suggest_categorical('gru_hidden_channels', [16, 32, 64])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.7)\n",
    "\n",
    "    model = EdgeGCN_GRU(hidden_channels=hidden_channels, gru_hidden_channels=gru_hidden_channels, out_channels=1, dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_model_path = \"gcn_gru.pth\"\n",
    "    for epoch in range(10):\n",
    "        train_loss = train(model, device, train_loader, optimizer, criterion)\n",
    "        val_loss, f1, precision, recall, auc, ks_statistic = evaluate(model, device, val_loader, criterion)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            # Save both model state and hyperparameters\n",
    "            checkpoint = {\n",
    "                'state_dict': model.state_dict(),\n",
    "                'hyperparameters': {\n",
    "                    'hidden_channels': hidden_channels,\n",
    "                    'gru_hidden_channels': gru_hidden_channels,\n",
    "                    'out_channels': 1,\n",
    "                    'dropout_rate': dropout_rate\n",
    "                },\n",
    "                'metrics': {\n",
    "                    'f1': f1,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'auc': auc,\n",
    "                    'ks_statistic': ks_statistic\n",
    "                }\n",
    "            }\n",
    "            torch.save(checkpoint, best_model_path)\n",
    "            shutil.copy(best_model_path, f\"/content/drive/My Drive/{best_model_path}\")\n",
    "\n",
    "    return best_f1  # Optimize for the best F1 score\n",
    "\n",
    "\n",
    "train_val_df, test_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df['Label']\n",
    ")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=train_val_df['Label']\n",
    ")\n",
    "\n",
    "train_processor = GraphDataProcessor(train_df)\n",
    "val_processor = GraphDataProcessor(val_df)\n",
    "test_processor = GraphDataProcessor(test_df)\n",
    "\n",
    "train_data = train_processor.prepare_graph_data()\n",
    "val_data = val_processor.prepare_graph_data()\n",
    "test_data = test_processor.prepare_graph_data()\n",
    "\n",
    "train_loader = DataLoader([train_data], batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader([val_data], batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader([test_data], batch_size=32, shuffle=False)\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\" Value (F1 Score): {trial.value}\")\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Load the best model to print all metrics\n",
    "checkpoint = torch.load(f\"/content/drive/My Drive/gcn_gru.pth\")\n",
    "metrics = checkpoint['metrics']\n",
    "print(\" Validation set metrics:\")\n",
    "print(f\"    F1 Score: {metrics['f1']}\")\n",
    "print(f\"    Precision: {metrics['precision']}\")\n",
    "print(f\"    Recall: {metrics['recall']}\")\n",
    "print(f\"    AUC: {metrics['auc']}\")\n",
    "print(f\"    KS Statistic: {metrics['ks_statistic']}\")\n",
    "model = EdgeGCN_GRU(\n",
    "    hidden_channels=checkpoint['hyperparameters']['hidden_channels'],\n",
    "    gru_hidden_channels=checkpoint['hyperparameters']['gru_hidden_channels'],\n",
    "    out_channels=1,\n",
    "    dropout_rate=checkpoint['hyperparameters']['dropout_rate']\n",
    ").to(device)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "test_loss, test_f1, test_precision, test_recall, test_auc, test_ks_statistic = evaluate(model, device, test_loader, criterion)\n",
    "\n",
    "print(\" Test set metrics:\")\n",
    "print(f\"    Loss: {test_loss}\")\n",
    "print(f\"    F1 Score: {test_f1}\")\n",
    "print(f\"    Precision: {test_precision}\")\n",
    "print(f\"    Recall: {test_recall}\")\n",
    "print(f\"    AUC: {test_auc}\")\n",
    "print(f\"    KS Statistic: {test_ks_statistic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBoMhUeoOpFE"
   },
   "source": [
    "## Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXmOWO-DOsJ8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.df['Time_step'] = pd.to_datetime(self.df['Time_step'])\n",
    "        self.df = self.df.sort_values(by=['Sender_Customer_Id', 'Time_step'])\n",
    "        self.df['Label'] = pd.to_numeric(self.df['Label'], errors='coerce').fillna(0).astype(int)\n",
    "        self.df['Days_Since_Last'] = self.df.groupby('Sender_Customer_Id')['Time_step'].diff().dt.days.fillna(0)\n",
    "\n",
    "        features = ['Transaction_Type', 'USD_Amount', 'risk_score', 'Days_Since_Last']\n",
    "        self.df = self.df[features + ['Label']]\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        self.df['Transaction_Type'] = le.fit_transform(self.df['Transaction_Type'])\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        self.df[['USD_Amount', 'risk_score', 'Days_Since_Last']] = scaler.fit_transform(self.df[['USD_Amount', 'risk_score', 'Days_Since_Last']])\n",
    "\n",
    "        X = self.df[features].values\n",
    "        y = self.df['Label'].values\n",
    "\n",
    "        return X, y\n",
    "\n",
    "def train_and_evaluate(X_train, y_train, X_val, y_val):\n",
    "    model = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
    "    model.fit(X_train)\n",
    "\n",
    "    # Predict anomalies on validation set\n",
    "    y_scores = -model.decision_function(X_val)\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred = np.where(y_pred == 1, 0, 1)  # Convert from {1, -1} to {0, 1}\n",
    "\n",
    "    # Evaluation metrics\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_scores)\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, y_scores)\n",
    "    ks_statistic = max(tpr - fpr)\n",
    "\n",
    "    return f1, precision, recall, auc, ks_statistic, model\n",
    "\n",
    "def test_model(model, X_test, y_test):\n",
    "    y_scores = -model.decision_function(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.where(y_pred == 1, 0, 1)  # Convert from {1, -1} to {0, 1}\n",
    "\n",
    "    # Evaluation metrics\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_scores)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "    ks_statistic = max(tpr - fpr)\n",
    "\n",
    "    return f1, precision, recall, auc, ks_statistic\n",
    "\n",
    "# Load your data\n",
    "# Assuming train_df, val_df, and test_df are already loaded DataFrames\n",
    "\n",
    "# Data processing\n",
    "train_processor = DataProcessor(train_df)\n",
    "val_processor = DataProcessor(val_df)\n",
    "test_processor = DataProcessor(test_df)\n",
    "\n",
    "X_train, y_train = train_processor.prepare_data()\n",
    "X_val, y_val = val_processor.prepare_data()\n",
    "X_test, y_test = test_processor.prepare_data()\n",
    "\n",
    "# Train and evaluate the model on validation set\n",
    "f1, precision, recall, auc, ks_statistic, trained_model = train_and_evaluate(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Print the validation metrics\n",
    "print(\" Validation set metrics:\")\n",
    "print(f\"    F1 Score: {f1}\")\n",
    "print(f\"    Precision: {precision}\")\n",
    "print(f\"    Recall: {recall}\")\n",
    "print(f\"    AUC: {auc}\")\n",
    "print(f\"    KS Statistic: {ks_statistic}\")\n",
    "\n",
    "# Test the model on the test set\n",
    "test_f1, test_precision, test_recall, test_auc, test_ks_statistic = test_model(trained_model, X_test, y_test)\n",
    "\n",
    "# Print the test metrics\n",
    "print(\" Test set metrics:\")\n",
    "print(f\"    F1 Score: {test_f1}\")\n",
    "print(f\"    Precision: {test_precision}\")\n",
    "print(f\"    Recall: {test_recall}\")\n",
    "print(f\"    AUC: {test_auc}\")\n",
    "print(f\"    KS Statistic: {test_ks_statistic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_4auKHdP8nb"
   },
   "source": [
    "## GAT with GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TG6ra4t3P_bn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import optuna\n",
    "import shutil\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class GraphDataProcessor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def undersample_df(self):\n",
    "        fraud_df = self.df[self.df['Label'] == 1]\n",
    "        non_fraud_df = self.df[self.df['Label'] == 0]\n",
    "        print(f\"Initial fraud cases: {len(fraud_df)}, non-fraud cases: {len(non_fraud_df)}\")\n",
    "        balanced_df = non_fraud_df.sample(len(fraud_df), random_state=42)\n",
    "        self.df = pd.concat([fraud_df, balanced_df])\n",
    "        print(f\"Balanced dataset: {len(self.df)} records\")\n",
    "\n",
    "    def prepare_graph_data(self):\n",
    "        self.undersample_df()\n",
    "        self.df['Time_step'] = pd.to_datetime(self.df['Time_step'])\n",
    "        self.df = self.df.sort_values(by=['Sender_Customer_Id', 'Time_step'])\n",
    "        self.df['Label'] = pd.to_numeric(self.df['Label'], errors='coerce').fillna(0).astype(int)\n",
    "        self.df['Days_Since_Last'] = self.df.groupby('Sender_Customer_Id')['Time_step'].diff().dt.days.fillna(0)\n",
    "\n",
    "        all_ids = pd.concat([self.df['Sender_Customer_Id'], self.df['Bene_Customer_Id']]).unique()\n",
    "        if len(all_ids) == 0:\n",
    "            raise ValueError(\"No unique IDs found in the dataset\")\n",
    "\n",
    "        id_map = {id: idx for idx, id in enumerate(all_ids)}\n",
    "        edge_index = torch.tensor([self.df['Sender_Customer_Id'].map(id_map).values, self.df['Bene_Customer_Id'].map(id_map).values], dtype=torch.long)\n",
    "\n",
    "        node_features = torch.zeros((len(all_ids), 1))\n",
    "        if self.df['Transaction_Type'].isnull().any() or self.df['USD_Amount'].isnull().any() or self.df['risk_score'].isnull().any():\n",
    "            raise ValueError(\"Null values found in essential columns\")\n",
    "\n",
    "        transaction_type_encoded = torch.tensor(LabelEncoder().fit_transform(self.df['Transaction_Type']), dtype=torch.float).view(-1, 1)\n",
    "        usd_amount = torch.tensor(StandardScaler().fit_transform(self.df[['USD_Amount']]), dtype=torch.float).view(-1, 1)\n",
    "        risk_score = torch.tensor(self.df['risk_score'].values, dtype=torch.float).view(-1, 1)\n",
    "        days_since_last = torch.tensor(StandardScaler().fit_transform(self.df[['Days_Since_Last']]), dtype=torch.float).view(-1, 1)\n",
    "\n",
    "        edge_attr = torch.cat([transaction_type_encoded, usd_amount, risk_score, days_since_last], dim=1)\n",
    "        edge_labels = torch.tensor(self.df['Label'].values, dtype=torch.long)\n",
    "\n",
    "        return Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=edge_labels)\n",
    "\n",
    "class EdgeGAT_GRU(nn.Module):\n",
    "    def __init__(self, hidden_channels, gru_hidden_channels, out_channels, dropout_rate, heads=1):\n",
    "        super(EdgeGAT_GRU, self).__init__()\n",
    "        self.conv1 = GATConv(1, hidden_channels, heads=heads)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels * heads)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=heads)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels * heads)\n",
    "        self.gru = nn.GRU(input_size=hidden_channels * 2 * heads + 4, hidden_size=gru_hidden_channels, batch_first=True)\n",
    "        self.lin = nn.Linear(gru_hidden_channels, out_channels)\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = F.dropout(F.relu(self.bn1(self.conv1(x, edge_index))), p=self.dropout_rate, training=self.training)\n",
    "        x = F.dropout(F.relu(self.bn2(self.conv2(x, edge_index))), p=self.dropout_rate, training=self.training)\n",
    "        sender_features = x[edge_index[0]]\n",
    "        receiver_features = x[edge_index[1]]\n",
    "        edge_features = torch.cat([sender_features, receiver_features, edge_attr], dim=1)\n",
    "        edge_features = edge_features.unsqueeze(0)  # Add batch dimension for GRU\n",
    "        gru_out, _ = self.gru(edge_features)\n",
    "        gru_out = gru_out.squeeze(0)\n",
    "        out = self.lin(gru_out)\n",
    "        return out.view(-1)\n",
    "\n",
    "def train(model, device, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.edge_attr)\n",
    "        loss = criterion(output, data.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, device, loader, criterion):\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_scores = [], [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data.x, data.edge_index, data.edge_attr)\n",
    "            loss = criterion(output, data.y.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.sigmoid(output).cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "\n",
    "            y_scores.extend(probs)\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(data.y.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    ks_statistic = max(tpr - fpr)\n",
    "\n",
    "    return total_loss / len(loader), f1, precision, recall, auc, ks_statistic\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    hidden_channels = trial.suggest_categorical('hidden_channels', [16, 32, 64])\n",
    "    gru_hidden_channels = trial.suggest_categorical('gru_hidden_channels', [16, 32, 64])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.7)\n",
    "    heads = trial.suggest_int('heads', 1, 4)\n",
    "\n",
    "    model = EdgeGAT_GRU(hidden_channels=hidden_channels, gru_hidden_channels=gru_hidden_channels, out_channels=1, dropout_rate=dropout_rate, heads=heads).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_model_path = \"gat_gru.pth\"\n",
    "    for epoch in range(10):\n",
    "        train_loss = train(model, device, train_loader, optimizer, criterion)\n",
    "        val_loss, f1, precision, recall, auc, ks_statistic = evaluate(model, device, val_loader, criterion)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            # Save both model state and hyperparameters\n",
    "            checkpoint = {\n",
    "                'state_dict': model.state_dict(),\n",
    "                'hyperparameters': {\n",
    "                    'hidden_channels': hidden_channels,\n",
    "                    'gru_hidden_channels': gru_hidden_channels,\n",
    "                    'out_channels': 1,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'heads': heads\n",
    "                },\n",
    "                'metrics': {\n",
    "                    'f1': f1,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'auc': auc,\n",
    "                    'ks_statistic': ks_statistic\n",
    "                }\n",
    "            }\n",
    "            torch.save(checkpoint, best_model_path)\n",
    "            shutil.copy(best_model_path, f\"/content/drive/My Drive/{best_model_path}\")\n",
    "\n",
    "    return best_f1  # Optimize for the best F1 score\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_val_df, test_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df['Label']\n",
    ")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=train_val_df['Label']\n",
    ")\n",
    "\n",
    "train_processor = GraphDataProcessor(train_df)\n",
    "val_processor = GraphDataProcessor(val_df)\n",
    "test_processor = GraphDataProcessor(test_df)\n",
    "\n",
    "train_data = train_processor.prepare_graph_data()\n",
    "val_data = val_processor.prepare_graph_data()\n",
    "test_data = test_processor.prepare_graph_data()\n",
    "\n",
    "train_loader = DataLoader([train_data], batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader([val_data], batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader([test_data], batch_size=32, shuffle=False)\n",
    "\n",
    "# Optuna study for hyperparameter optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\" Value (F1 Score): {trial.value}\")\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Load the best model to print all metrics\n",
    "checkpoint = torch.load(f\"/content/drive/My Drive/gat_gru.pth\")\n",
    "metrics = checkpoint['metrics']\n",
    "print(\" Validation set metrics:\")\n",
    "print(f\"    F1 Score: {metrics['f1']}\")\n",
    "print(f\"    Precision: {metrics['precision']}\")\n",
    "print(f\"    Recall: {metrics['recall']}\")\n",
    "print(f\"    AUC: {metrics['auc']}\")\n",
    "print(f\"    KS Statistic: {metrics['ks_statistic']}\")\n",
    "\n",
    "model = EdgeGAT_GRU(\n",
    "    hidden_channels=checkpoint['hyperparameters']['hidden_channels'],\n",
    "    gru_hidden_channels=checkpoint['hyperparameters']['gru_hidden_channels'],\n",
    "    out_channels=1,\n",
    "    dropout_rate=checkpoint['hyperparameters']['dropout_rate'],\n",
    "    heads=checkpoint['hyperparameters']['heads']\n",
    ").to(device)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "test_loss, test_f1, test_precision, test_recall, test_auc, test_ks_statistic = evaluate(model, device, test_loader, criterion)\n",
    "\n",
    "print(\" Test set metrics:\")\n",
    "print(f\"    Loss: {test_loss}\")\n",
    "print(f\"    F1 Score: {test_f1}\")\n",
    "print(f\"    Precision: {test_precision}\")\n",
    "print(f\"    Recall: {test_recall}\")\n",
    "print(f\"    AUC: {test_auc}\")\n",
    "print(f\"    KS Statistic: {test_ks_statistic}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

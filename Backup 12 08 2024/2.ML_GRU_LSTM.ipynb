{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import shutil  # for moving files\n",
    "\n",
    "class EdgeGCN_LSTM(nn.Module):\n",
    "    def __init__(self, hidden_channels, lstm_hidden_channels, out_channels, dropout_rate, num_layers, l2_lambda):\n",
    "        super(EdgeGCN_LSTM, self).__init__()\n",
    "        self.conv1 = GCNConv(1, hidden_channels)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_channels * 2 + 3,\n",
    "            hidden_size=lstm_hidden_channels,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        self.lin1 = nn.Linear(lstm_hidden_channels, lstm_hidden_channels // 2)\n",
    "        self.lin2 = nn.Linear(lstm_hidden_channels // 2, out_channels)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = F.dropout(F.relu(self.bn1(self.conv1(x, edge_index))), p=self.dropout_rate, training=self.training)\n",
    "        x = F.dropout(F.relu(self.bn2(self.conv2(x, edge_index))), p=self.dropout_rate, training=self.training)\n",
    "        sender_features = x[edge_index[0]]\n",
    "        receiver_features = x[edge_index[1]]\n",
    "        edge_features = torch.cat([sender_features, receiver_features, edge_attr], dim=1)\n",
    "        edge_features = edge_features.unsqueeze(0)\n",
    "        lstm_out, _ = self.lstm(edge_features)\n",
    "        lstm_out = lstm_out.squeeze(0)\n",
    "        out = F.relu(self.lin1(lstm_out))\n",
    "        out = self.lin2(out)\n",
    "        return out.view(-1)\n",
    "\n",
    "# Function to load the most recent CSV file from 'Result' directory\n",
    "def load_most_recent_csv(directory):\n",
    "    list_of_files = glob.glob(f'{directory}/*.csv')  # * means all if need specific format then *.csv\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    return latest_file\n",
    "\n",
    "# Load the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EdgeGCN_LSTM(hidden_channels=32, lstm_hidden_channels=64, out_channels=1, dropout_rate=0.3, num_layers=2, l2_lambda=0.001)\n",
    "model.load_state_dict(torch.load('gcn_lstm_model.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Process and predict function\n",
    "def process_and_predict(filename, output_directory):\n",
    "    # Load data\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Preprocess data as per your requirement\n",
    "    df['Transaction_Type'] = LabelEncoder().fit_transform(df['Transaction_Type'])\n",
    "    df['USD_Amount'] = StandardScaler().fit_transform(df[['USD_Amount']])\n",
    "    df['risk_score'] = StandardScaler().fit_transform(df[['risk_score']])\n",
    "    \n",
    "    # Prepare the graph data\n",
    "    all_ids = pd.concat([df['Sender_Customer_Id'], df['Bene_Customer_Id']]).unique()\n",
    "    id_map = {id: idx for idx, id in enumerate(all_ids)}\n",
    "    edge_index = torch.tensor(\n",
    "        np.vstack([\n",
    "            df['Sender_Customer_Id'].map(id_map).values,\n",
    "            df['Bene_Customer_Id'].map(id_map).values\n",
    "        ]), dtype=torch.long)\n",
    "    node_features = torch.zeros((len(all_ids), 1))\n",
    "    edge_attr = torch.tensor(df[['Transaction_Type', 'USD_Amount', 'risk_score']].values, dtype=torch.float)\n",
    "\n",
    "    test_data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    test_loader = DataLoader([test_data], batch_size=1)\n",
    "\n",
    "    # Predict\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data.x, data.edge_index, data.edge_attr)\n",
    "            prediction = torch.sigmoid(output).cpu().numpy()\n",
    "            predictions.append(prediction[0])\n",
    "\n",
    "    # Save predictions back to CSV\n",
    "    df['Predictions'] = predictions\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "    # Move the file to 'Result_Done' directory\n",
    "    os.makedirs(output_directory, exist_ok=True)  # Ensure the directory exists\n",
    "    shutil.move(filename, os.path.join(output_directory, os.path.basename(filename)))\n",
    "\n",
    "# Main execution\n",
    "directory = 'Result'\n",
    "output_directory = 'Result_Done'\n",
    "latest_file = load_most_recent_csv(directory)\n",
    "process_and_predict(latest_file, output_directory)\n",
    "print(f\"File {os.path.basename(latest_file)} processed and moved to {output_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 2.ML_GRU_LSTM.ipynb to script\n",
      "[NbConvertApp] Writing 5534 bytes to 2.ML_GRU_LSTM.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script 2.ML_GRU_LSTM.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 189 files in the directory 'Result/'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_directory(directory):\n",
    "    # List all entries in the directory\n",
    "    entries = os.listdir(directory)\n",
    "    # Filter out directory names, only count files\n",
    "    files = [entry for entry in entries if os.path.isfile(os.path.join(directory, entry))]\n",
    "    return len(files)\n",
    "\n",
    "# Specify the directory you want to check\n",
    "directory_path = 'Result/'\n",
    "number_of_files = count_files_in_directory(directory_path)\n",
    "print(f\"There are {number_of_files} files in the directory '{directory_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 3787\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to calculate the number of batches\n",
    "def calculate_batches(filename, batch_size=100):\n",
    "    # Read the CSV into a DataFrame\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Calculate the number of batches\n",
    "    total_rows = len(df)\n",
    "    number_of_batches = (total_rows + batch_size - 1) // batch_size  # This uses integer division to round up\n",
    "    \n",
    "    return number_of_batches\n",
    "\n",
    "# Example usage\n",
    "filename = 'Thesis/test.csv'\n",
    "num_batches = calculate_batches(filename)\n",
    "print(f\"Number of batches: {num_batches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics DataFrame:\n",
      "   Precision   Recall       F1  AUC  KS_Statistic  KS_PValue\n",
      "0    0.38386  0.38386  0.38386  0.5           NaN        NaN\n",
      "1    0.58682  0.58682  0.58682  0.5           NaN        NaN\n",
      "2    0.34556  0.34556  0.34556  0.5           NaN        NaN\n",
      "3    0.34322  0.34322  0.34322  0.5           NaN        NaN\n",
      "4    0.47920  0.47920  0.47920  0.5           NaN        NaN\n",
      "\n",
      "Fuzzy Result DataFrame:\n",
      "             Time_step  Label Transaction_Id           Sender_Account  \\\n",
      "0  2024-01-21 06:24:00      0     T-90966-08                      NaN   \n",
      "1  2024-01-21 06:24:00      0    T-930664-06                      NaN   \n",
      "2  2024-01-21 06:24:00      1    T-197980-00  DIGITAL-MONEY-197559-00   \n",
      "3  2024-01-21 06:24:00      1    T-197980-00  DIGITAL-MONEY-197559-00   \n",
      "4  2024-01-21 06:28:00      1    T-295862-00  DIGITAL-MONEY-295444-00   \n",
      "\n",
      "  Sender_Institution Sender_Country  USD_Amount        Bene_Account  \\\n",
      "0                NaN            NaN     9500.58   CHECKING-90551-08   \n",
      "1                NaN            NaN     4292.05  CHECKING-930186-06   \n",
      "2      JPMORGANCHASE         France   500091.00  CHECKING-197561-00   \n",
      "3      JPMORGANCHASE         France   500091.00  CHECKING-197561-00   \n",
      "4     BANK-295641-00  United States    59539.00  CHECKING-295445-00   \n",
      "\n",
      "  Bene_Institution    Bene_Country  ...  Sender_Name_Masked  \\\n",
      "0    JPMORGANCHASE   United States  ...                 NaN   \n",
      "1    JPMORGANCHASE   United States  ...                 NaN   \n",
      "2    JPMORGANCHASE   United States  ...  William Raymond 53   \n",
      "3    JPMORGANCHASE   United States  ...  William Raymond 53   \n",
      "4    JPMORGANCHASE  United-Kingdom  ...       April King 71   \n",
      "\n",
      "   Sender_Account_Number_Masked  Bene_Is_Pep Bene_Customer_Id  \\\n",
      "0                           NaN            0           753504   \n",
      "1                           NaN            0           229946   \n",
      "2         JPMC-CLIENT-5848-5714            1           674936   \n",
      "3         JPMC-CLIENT-5848-5714            0           481007   \n",
      "4         JPMC-CLIENT-9682-2825            0           446128   \n",
      "\n",
      "      Bene_Name_Masked  Bene_Account_Number_Masked  risk_score  \\\n",
      "0  Thomas Frederick 26       JPMC-CLIENT-7327-1995   64.315659   \n",
      "1   Vanessa Spencer 37       JPMC-CLIENT-5805-8589   64.315659   \n",
      "2        Tara Weiss 80          CUSTOMER-9117-4726   64.315659   \n",
      "3       Andrew Carr 33          CUSTOMER-6753-7317   64.315659   \n",
      "4         Joe Green 83          CUSTOMER-5481-8388   64.315659   \n",
      "\n",
      "                                        fuzzy_result Predictions  \\\n",
      "0                           Cross-border transaction    0.476377   \n",
      "1                           Cross-border transaction    0.477668   \n",
      "2  High-risk transaction type (crypto or payment)...    0.477266   \n",
      "3  High-risk transaction type (crypto or payment)...    0.476947   \n",
      "4  High-risk transaction type (crypto or payment)...    0.476786   \n",
      "\n",
      "   Label_Prediction  \n",
      "0                 0  \n",
      "1                 0  \n",
      "2                 0  \n",
      "3                 0  \n",
      "4                 0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Replace this with the path to your directory containing the CSV files\n",
    "directory_path = 'Hybrid_GCN_LSTM'\n",
    "\n",
    "# Function to merge files with a specific prefix\n",
    "def merge_csv_files(directory, prefix):\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dataframes = []\n",
    "    \n",
    "    # Loop through each file in the directory\n",
    "    for file in os.listdir(directory):\n",
    "        # Check if the file name starts with the specified prefix\n",
    "        if file.startswith(prefix):\n",
    "            # Create a DataFrame from the CSV file\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Append the DataFrame to the list\n",
    "            dataframes.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames in the list\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return merged_df\n",
    "\n",
    "# Merge files starting with 'metrics' and 'fuzzy_result'\n",
    "metrics_df = merge_csv_files(directory_path, 'Metrics')\n",
    "fuzzy_result_df = merge_csv_files(directory_path, 'Fuzzy_Results')\n",
    "\n",
    "# Print the first few rows of each DataFrame to verify\n",
    "print(\"Metrics DataFrame:\")\n",
    "print(metrics_df.head())\n",
    "print(\"\\nFuzzy Result DataFrame:\")\n",
    "print(fuzzy_result_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages of each column:\n",
      "Precision       0.477106\n",
      "Recall          0.477106\n",
      "F1              0.477106\n",
      "AUC             0.500000\n",
      "KS_Statistic         NaN\n",
      "KS_PValue            NaN\n",
      "dtype: float64\n",
      "Confusion Matrix:\n",
      "[[193913      0]\n",
      " [179766      0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.0\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "AUC: 0.5692677568014398\n",
      "KS Statistic: 0.14003083533604044\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, roc_curve,confusion_matrix\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "\n",
    "def calculate_statistics(df):\n",
    "    # Calculate average of each column\n",
    "    averages = df.mean()\n",
    "    print(\"Averages of each column:\")\n",
    "    print(averages)\n",
    "    \n",
    "def evaluate_model(y_true, y_scores, threshold=0.5):\n",
    "    # Apply threshold to convert probabilities to binary predictions\n",
    "    y_pred = (y_scores > threshold).astype(int)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    # Calculate F1, Precision, and Recall\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "\n",
    "    # Calculate AUC using the probability scores\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "    # Calculate the ROC curve to derive the KS Statistic\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    ks_statistic = max(tpr - fpr)\n",
    "\n",
    "    # Print results\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"AUC:\", auc)\n",
    "    print(\"KS Statistic:\", ks_statistic)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate and display statistics for 'metrics' DataFrame\n",
    "calculate_statistics(metrics_df)\n",
    "\n",
    "# Evaluate model for 'fuzzy_result' DataFrame\n",
    "evaluate_model(fuzzy_result_df['Label'], fuzzy_result_df['Predictions'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial fraud cases: 46507, non-fraud cases: 274326\n",
      "Balanced dataset: 93014 records\n",
      "Initial fraud cases: 15503, non-fraud cases: 91442\n",
      "Balanced dataset: 31006 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/echristi/.local/lib/python3.6/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "\u001b[32m[I 2024-08-22 09:58:36,584]\u001b[0m A new study created in memory with name: no-name-5a495783-7782-4228-bb80-babef13a25b6\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:148: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:151: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "\u001b[33m[W 2024-08-22 09:58:36,589]\u001b[0m Trial 0 failed because of the following error: AttributeError(\"module 'torch.nn.parameter' has no attribute 'UninitializedParameter'\",)\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/echristi/.local/lib/python3.6/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<ipython-input-7-5cfbcb8d1b3c>\", line 154, in objective\n",
      "    model = EdgeGCN_LSTM(hidden_channels=hidden_channels, lstm_hidden_channels=lstm_hidden_channels, out_channels=1, dropout_rate=dropout_rate).to(device)\n",
      "  File \"<ipython-input-7-5cfbcb8d1b3c>\", line 23, in __init__\n",
      "    self.conv1 = GCNConv(1, hidden_channels)\n",
      "  File \"/home/echristi/.local/lib/python3.6/site-packages/torch_geometric/nn/conv/gcn_conv.py\", line 140, in __init__\n",
      "    weight_initializer='glorot')\n",
      "  File \"/home/echristi/.local/lib/python3.6/site-packages/torch_geometric/nn/dense/linear.py\", line 65, in __init__\n",
      "    self.reset_parameters()\n",
      "  File \"/home/echristi/.local/lib/python3.6/site-packages/torch_geometric/nn/dense/linear.py\", line 78, in reset_parameters\n",
      "    if isinstance(self.weight, nn.parameter.UninitializedParameter):\n",
      "AttributeError: module 'torch.nn.parameter' has no attribute 'UninitializedParameter'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn.parameter' has no attribute 'UninitializedParameter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5cfbcb8d1b3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;31m# Create a study object and run the optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;31m# Save the best model at the end of all trials to the desired local path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         )\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     ):\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-5cfbcb8d1b3c>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;31m# Model initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEdgeGCN_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_hidden_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_hidden_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-5cfbcb8d1b3c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hidden_channels, lstm_hidden_channels, out_channels, dropout_rate)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEdgeGCN_LSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Use a single GCN layer followed by LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_channels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_hidden_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_hidden_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, improved, cached, add_self_loops, normalize, bias, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         self.lin = Linear(in_channels, out_channels, bias=False,\n\u001b[0;32m--> 140\u001b[0;31m                           weight_initializer='glorot')\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch_geometric/nn/dense/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, bias, weight_initializer, bias_initializer)\u001b[0m\n\u001b[1;32m     63\u001b[0m             self._lazy_load_hook)\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__deepcopy__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch_geometric/nn/dense/linear.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUninitializedParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_initializer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'glorot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn.parameter' has no attribute 'UninitializedParameter'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('Thesis/train_with_fuzzy_results2.csv')\n",
    "\n",
    "class EdgeGCN_LSTM(nn.Module):\n",
    "    def __init__(self, hidden_channels, lstm_hidden_channels, out_channels, dropout_rate):\n",
    "        super(EdgeGCN_LSTM, self).__init__()\n",
    "        # Use a single GCN layer followed by LSTM\n",
    "        self.conv1 = GCNConv(1, hidden_channels)\n",
    "        self.lstm = nn.LSTM(input_size=hidden_channels * 2 + 3, hidden_size=lstm_hidden_channels, batch_first=True)\n",
    "        self.lin = nn.Linear(lstm_hidden_channels, out_channels)\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Apply GCN and dropout\n",
    "        x = F.dropout(F.relu(self.conv1(x, edge_index)), p=self.dropout_rate, training=self.training)\n",
    "        \n",
    "        # Prepare features for LSTM\n",
    "        sender_features = x[edge_index[0]]\n",
    "        receiver_features = x[edge_index[1]]\n",
    "        edge_features = torch.cat([sender_features, receiver_features, edge_attr], dim=1)\n",
    "        \n",
    "        # Process with LSTM\n",
    "        edge_features = edge_features.unsqueeze(0)  # Add batch dimension for LSTM\n",
    "        lstm_out, _ = self.lstm(edge_features)\n",
    "        lstm_out = lstm_out.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        # Linear output layer\n",
    "        out = self.lin(lstm_out)\n",
    "        return out.view(-1)\n",
    "\n",
    "\n",
    "class GraphDataProcessor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    def undersample_df(self):\n",
    "        fraud_df = self.df[self.df['Label'] == 1]\n",
    "        non_fraud_df = self.df[self.df['Label'] == 0]\n",
    "        print(f\"Initial fraud cases: {len(fraud_df)}, non-fraud cases: {len(non_fraud_df)}\")\n",
    "\n",
    "          # Check if there are enough fraud cases to sample\n",
    "        if len(fraud_df) < len(non_fraud_df):\n",
    "            balanced_df = non_fraud_df.sample(len(fraud_df), random_state=42)\n",
    "        else:\n",
    "            balanced_df = non_fraud_df\n",
    "\n",
    "        self.df = pd.concat([fraud_df, balanced_df]).sample(frac=1)  # shuffle the dataset\n",
    "        print(f\"Balanced dataset: {len(self.df)} records\")\n",
    "\n",
    "    def prepare_graph_data(self):\n",
    "        self.undersample_df()\n",
    "        self.df['Time_step'] = pd.to_datetime(self.df['Time_step'])\n",
    "        self.df = self.df.sort_values(by=['Sender_Customer_Id', 'Time_step'])\n",
    "        self.df['Label'] = pd.to_numeric(self.df['Label'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "        all_ids = pd.concat([self.df['Sender_Customer_Id'], self.df['Bene_Customer_Id']]).unique()\n",
    "        if len(all_ids) == 0:\n",
    "            raise ValueError(\"No unique IDs found in the dataset\")\n",
    "\n",
    "        id_map = {id: idx for idx, id in enumerate(all_ids)}\n",
    "        edge_index = torch.tensor([self.df['Sender_Customer_Id'].map(id_map).values, self.df['Bene_Customer_Id'].map(id_map).values], dtype=torch.long)\n",
    "\n",
    "        node_features = torch.zeros((len(all_ids), 1))\n",
    "      \n",
    "        transaction_type_encoded = torch.tensor(LabelEncoder().fit_transform(self.df['Transaction_Type']), dtype=torch.float).view(-1, 1)\n",
    "        usd_amount = torch.tensor(StandardScaler().fit_transform(self.df[['USD_Amount']]), dtype=torch.float).view(-1, 1)\n",
    "        risk_score = torch.tensor(self.df['risk_score'].values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "        edge_attr = torch.cat([transaction_type_encoded, usd_amount, risk_score], dim=1)\n",
    "        edge_labels = torch.tensor(self.df['Label'].values, dtype=torch.long)\n",
    "\n",
    "        return Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=edge_labels)\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=train_df['Label']\n",
    ")\n",
    "\n",
    "train_processor = GraphDataProcessor(train_df)\n",
    "val_processor = GraphDataProcessor(val_df)\n",
    "\n",
    "train_data = train_processor.prepare_graph_data()\n",
    "val_data = val_processor.prepare_graph_data()\n",
    "\n",
    "train_loader = DataLoader([train_data], batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader([val_data], batch_size=32, shuffle=False)\n",
    "\n",
    "def train(model, device, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.edge_attr)\n",
    "        loss = criterion(output, data.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, device, loader, criterion):\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_scores = [], [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data.x, data.edge_index, data.edge_attr)\n",
    "            loss = criterion(output, data.y.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.sigmoid(output).cpu().numpy()\n",
    "            preds = (probs > 0.4).astype(int)\n",
    "\n",
    "            y_scores.extend(probs)\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(data.y.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    ks_statistic = max(tpr - fpr)\n",
    "\n",
    "    return total_loss / len(loader), f1, precision, recall, auc, ks_statistic\n",
    "def objective(trial):\n",
    "    global best_f1, best_model_checkpoint\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    hidden_channels = trial.suggest_categorical('hidden_channels', [16, 32, 64])\n",
    "    lstm_hidden_channels = trial.suggest_categorical('lstm_hidden_channels', [16, 32, 64])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.7)\n",
    "\n",
    "    # Model initialization\n",
    "    model = EdgeGCN_LSTM(hidden_channels=hidden_channels, lstm_hidden_channels=lstm_hidden_channels, out_channels=1, dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Training and validation\n",
    "    for epoch in range(10):\n",
    "        train_loss = train(model, device, train_loader, optimizer, criterion)\n",
    "        val_loss, f1, precision, recall, auc, ks_statistic = evaluate(model, device, val_loader, criterion)\n",
    "\n",
    "        # Check if the current model is the best one; save it if true\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model_checkpoint = {\n",
    "                'state_dict': model.state_dict(),\n",
    "                'hyperparameters': {\n",
    "                    'lr': lr,\n",
    "                    'hidden_channels': hidden_channels,\n",
    "                    'lstm_hidden_channels': lstm_hidden_channels,\n",
    "                    'dropout_rate': dropout_rate\n",
    "                },\n",
    "                'metrics': {\n",
    "                    'f1': f1,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'auc': auc,\n",
    "                    'ks_statistic': ks_statistic\n",
    "                }\n",
    "            }\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Initialize variables to store best model details\n",
    "best_f1 = 0\n",
    "best_model_checkpoint = None\n",
    "\n",
    "# Create a study object and run the optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Save the best model at the end of all trials to the desired local path\n",
    "best_model_path = \"Thesis/gsm_lstm_model.pth\"\n",
    "if best_model_checkpoint:\n",
    "    torch.save(best_model_checkpoint, best_model_path)\n",
    "    print(f\"Best model saved at: {best_model_path}\")\n",
    "\n",
    "# Output the results of the best trial\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\" Value (F1 Score): {trial.value}\")\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "# Load the best model's checkpoint\n",
    "checkpoint = torch.load(best_model_path)\n",
    "\n",
    "# Access and print the validation metrics stored in the checkpoint\n",
    "metrics = checkpoint['metrics']\n",
    "print(\"Validation set metrics:\")\n",
    "print(f\"    F1 Score: {metrics['f1']}\")\n",
    "print(f\"    Precision: {metrics['precision']}\")\n",
    "print(f\"    Recall: {metrics['recall']}\")\n",
    "print(f\"    AUC: {metrics['auc']}\")\n",
    "print(f\"    KS Statistic: {metrics['ks_statistic']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 320833 entries, 269273 to 82844\n",
      "Data columns (total 21 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   Time_step                     320833 non-null  object \n",
      " 1   Label                         320833 non-null  int64  \n",
      " 2   Transaction_Id                320833 non-null  object \n",
      " 3   Sender_Account                257106 non-null  object \n",
      " 4   Sender_Institution            257106 non-null  object \n",
      " 5   Sender_Country                257106 non-null  object \n",
      " 6   USD_Amount                    320833 non-null  float64\n",
      " 7   Bene_Account                  320833 non-null  object \n",
      " 8   Bene_Institution              320833 non-null  object \n",
      " 9   Bene_Country                  320833 non-null  object \n",
      " 10  Transaction_Type              320833 non-null  object \n",
      " 11  Sender_Is_Pep                 320833 non-null  int64  \n",
      " 12  Sender_Customer_Id            320833 non-null  int64  \n",
      " 13  Sender_Name_Masked            257106 non-null  object \n",
      " 14  Sender_Account_Number_Masked  257106 non-null  object \n",
      " 15  Bene_Is_Pep                   320833 non-null  int64  \n",
      " 16  Bene_Customer_Id              320833 non-null  int64  \n",
      " 17  Bene_Name_Masked              320833 non-null  object \n",
      " 18  Bene_Account_Number_Masked    320833 non-null  object \n",
      " 19  risk_score                    320833 non-null  float64\n",
      " 20  fuzzy_result                  320833 non-null  object \n",
      "dtypes: float64(2), int64(5), object(14)\n",
      "memory usage: 53.9+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('Thesis/train_with_fuzzy_results2.csv')\n",
    "\n",
    "class EdgeGAT_LSTM(nn.Module):\n",
    "    def __init__(self, hidden_channels, lstm_hidden_channels, out_channels, dropout_rate, num_layers, l2_lambda, num_heads):\n",
    "        super(EdgeGAT_LSTM, self).__init__()\n",
    "        self.conv1 = GATConv(1, hidden_channels, heads=num_heads, dropout=dropout_rate)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels * num_heads)  # Adjust the size for the number of heads\n",
    "        self.conv2 = GATConv(hidden_channels * num_heads, hidden_channels, heads=num_heads, dropout=dropout_rate)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels * num_heads)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_channels * num_heads * 2 + 3,  # Adjust input size to accommodate GAT output\n",
    "            hidden_size=lstm_hidden_channels,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        self.lin1 = nn.Linear(lstm_hidden_channels, lstm_hidden_channels // 2)\n",
    "        self.lin2 = nn.Linear(lstm_hidden_channels // 2, out_channels)\n",
    "        self.dropout_rate = dropout_rate  # This line was missing, defining it as an instance attribute\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = F.dropout(F.relu(self.bn1(self.conv1(x, edge_index))), p=self.dropout_rate, training=self.training)\n",
    "        x = F.dropout(F.relu(self.bn2(self.conv2(x, edge_index))), p=self.dropout_rate, training=self.training)\n",
    "        sender_features = x[edge_index[0]]\n",
    "        receiver_features = x[edge_index[1]]\n",
    "        edge_features = torch.cat([sender_features, receiver_features, edge_attr], dim=1)\n",
    "        edge_features = edge_features.unsqueeze(0)\n",
    "        lstm_out, _ = self.lstm(edge_features)\n",
    "        lstm_out = lstm_out.squeeze(0)\n",
    "        out = F.relu(self.lin1(lstm_out))\n",
    "        out = self.lin2(out)\n",
    "        return out.view(-1)\n",
    "\n",
    "\n",
    "class GraphDataProcessor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def undersample_df(self):\n",
    "        fraud_df = self.df[self.df['Label'] == 1]\n",
    "        non_fraud_df = self.df[self.df['Label'] == 0]\n",
    "        balanced_df = non_fraud_df.sample(len(fraud_df), random_state=42)\n",
    "        self.df = pd.concat([fraud_df, balanced_df])\n",
    "\n",
    "    def prepare_graph_data(self):\n",
    "        self.undersample_df()\n",
    "        self.df['Time_step'] = pd.to_datetime(self.df['Time_step'])\n",
    "        self.df = self.df.sort_values(by=['Sender_Customer_Id', 'Time_step'])\n",
    "\n",
    "        self.df['Label'] = pd.to_numeric(self.df['Label'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "        all_ids = pd.concat([self.df['Sender_Customer_Id'], self.df['Bene_Customer_Id']]).unique()\n",
    "        id_map = {id: idx for idx, id in enumerate(all_ids)}\n",
    "        edge_index = torch.tensor(\n",
    "            np.vstack([\n",
    "                self.df['Sender_Customer_Id'].map(id_map).values,\n",
    "                self.df['Bene_Customer_Id'].map(id_map).values\n",
    "            ]), dtype=torch.long)\n",
    "        node_features = torch.zeros((len(all_ids), 1))\n",
    "\n",
    "        edge_attr = torch.cat([\n",
    "            torch.tensor(LabelEncoder().fit_transform(self.df['Transaction_Type']), dtype=torch.float).view(-1, 1),\n",
    "            torch.tensor(StandardScaler().fit_transform(self.df[['USD_Amount']]), dtype=torch.float).view(-1, 1),\n",
    "            torch.tensor(self.df['risk_score'].values, dtype=torch.float).view(-1, 1)\n",
    "        ], dim=1)\n",
    "        edge_labels = torch.tensor(self.df['Label'].values, dtype=torch.long)\n",
    "        return Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=edge_labels)\n",
    "\n",
    "def train(model, device, loader, optimizer, criterion, l2_lambda):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.edge_attr)\n",
    "        loss = criterion(output, data.y.float())\n",
    "        l2_reg = torch.tensor(0., requires_grad=True)\n",
    "        for param in model.parameters():\n",
    "            l2_reg = l2_reg + torch.norm(param)\n",
    "        loss = loss + l2_lambda * l2_reg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, device, loader, criterion):\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_scores = [], [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data.x, data.edge_index, data.edge_attr)\n",
    "            loss = criterion(output, data.y.float())\n",
    "            total_loss += loss.item()\n",
    "            probs = torch.sigmoid(output).cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            y_scores.extend(probs)\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(data.y.cpu().numpy())\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    return total_loss / len(loader), f1, precision, recall, auc\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    hidden_channels = trial.suggest_categorical('hidden_channels', [16, 32, 64])\n",
    "    lstm_hidden_channels = trial.suggest_categorical('lstm_hidden_channels', [16, 32, 64, 128])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.7)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    l2_lambda = trial.suggest_loguniform('l2_lambda', 1e-6, 1e-2)\n",
    "    num_heads = trial.suggest_int('num_heads', 1, 8)\n",
    "\n",
    "    model = EdgeGAT_LSTM(hidden_channels=hidden_channels, lstm_hidden_channels=lstm_hidden_channels,\n",
    "                         out_channels=1, dropout_rate=dropout_rate, num_layers=num_layers, l2_lambda=l2_lambda, num_heads=num_heads).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Use train data only\n",
    "    train_processor = GraphDataProcessor(train_df)\n",
    "    train_data = train_processor.prepare_graph_data()\n",
    "    train_loader = DataLoader([train_data], batch_size=32, shuffle=True)\n",
    "\n",
    "    best_f1 = 0\n",
    "    for epoch in range(10):  # Adjust the number of epochs if needed\n",
    "        train_loss = train(model, device, train_loader, optimizer, criterion, l2_lambda)\n",
    "        _, train_f1, _, _, _ = evaluate(model, device, train_loader, criterion)\n",
    "        if train_f1 > best_f1:\n",
    "            best_f1 = train_f1\n",
    "    return best_f1\n",
    "\n",
    "# Optuna study to find best hyperparameters\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\" Value (F1 Score): {trial.value}\")\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Retrain with the best hyperparameters and save the model\n",
    "best_params = trial.params\n",
    "model = EdgeGAT_LSTM(\n",
    "    hidden_channels=best_params['hidden_channels'],\n",
    "    lstm_hidden_channels=best_params['lstm_hidden_channels'],\n",
    "    out_channels=1,\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    l2_lambda=best_params['l2_lambda'],\n",
    "    num_heads=best_params['num_heads']\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Recreate the DataLoader after hyperparameter tuning\n",
    "train_processor = GraphDataProcessor(train_df)\n",
    "train_data = train_processor.prepare_graph_data()\n",
    "train_loader = DataLoader([train_data], batch_size=32, shuffle=True)\n",
    "\n",
    "# Train the final model\n",
    "for epoch in range(10):  # Adjust the number of epochs if needed\n",
    "    train_loss = train(model, device, train_loader, optimizer, criterion, best_params['l2_lambda'])\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'hyperparameters': {\n",
    "        'hidden_channels': best_params['hidden_channels'],\n",
    "        'lstm_hidden_channels': best_params['lstm_hidden_channels'],\n",
    "        'out_channels': 1,\n",
    "        'dropout_rate': best_params['dropout_rate'],\n",
    "        'num_layers': best_params['num_layers'],\n",
    "        'l2_lambda': best_params['l2_lambda'],\n",
    "        'lr': best_params['lr'],\n",
    "        'num_heads': best_params['num_heads']\n",
    "    }\n",
    "}, 'hybrid_gat_lstm_model.pth')\n",
    "print(\"Model saved as 'hybrid_gat_lstm_model.pth'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
